- schema: "exampleop.overloadname(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  autocompare: disable
  register_op: False # Whether generate registe code for this op, default value is True
  print_func_call_info: False # whether generate code that prints function call information
  print_op_args: True # whether generate code that prints op args
  dummy_call_diopi: False # Does not generate code that actually calls the diopi function, defalut value is False
  custom_code_at_the_beginning: "/* Here can be a piece of c++ code at the begining*/"
  custom_code_before_call_diopi: |
    std::cout << "self:" << self << std::endl;
    std::cout << "other:" << other << std::endl;
  custom_code_before_return: |
    dipu::getCurrentDIPUStream().synchronize();
    std::cout << "out:" << out << std::endl;
  interface: diopiAddScalar(ctx, out, self, other, alpha)

- schema: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
  interface: diopiFill(ctx, self, value)

- schema: "aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAddScalar(ctx, out, self, other, alpha)

- schema: "add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    TORCH_CHECK(!( (c10::isFloatingType(other.type())) && (!c10::isFloatingType(self.scalar_type())) ), __FUNCTION__, ":", __FILE__, ":", __LINE__,
        " result type Float can't be cast to the desired output type Int");
  interface: diopiAddInpScalar(ctx, self, other, alpha)

- schema: "add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    TORCH_CHECK(!( (c10::isFloatingType(other.scalar_type())) && (!c10::isFloatingType(self.scalar_type())) ), __FUNCTION__, ":", __FILE__, ":", __LINE__,
        " result type Float can't be cast to the desired output type Int");
    if (other.numel() == 1) {
        return dipu_add__scalar(self, other.cpu().item(), alpha);
    }
  interface: diopiAddInp(ctx, self, other, alpha)

- schema: "aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1) {
        return dipu_add_scalar_out(self, other.cpu().item(), alpha, out);
    } else if (self.numel() == 1) {
        if (alpha.toDouble() == 1.0) {
          return dipu_add_scalar_out(other, self.cpu().item(), alpha, out);
        } else {
          dipu_fill__scalar(out, self.cpu().item());
          return dipu_add__tensor(out, other, alpha);
        }
    }
  interface: diopiAdd(ctx, out, self, other, alpha)

- schema: "aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSubScalar(ctx, out, self, other, alpha)

- schema: "aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_sub_scalar_out(self, other.item(), alpha, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        at::Tensor selfTensor = at::empty_like(other);
        dipu_fill__scalar(selfTensor, self.item());
        return dipu_sub_out(selfTensor, other, alpha, out);
    }
  interface: diopiSub(ctx, out, self, other, alpha)

- schema: "sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)"
  interface: diopiSubInpScalar(ctx, self, other, alpha)

- schema: "sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_sub__scalar(self, other.item(), alpha);
    }
  interface: diopiSubInp(ctx, self, other, alpha)

- schema: "div.Scalar(Tensor self, Scalar other) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiDivScalar(ctx, out, self, other, RoundModeNone)

- schema: "div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiDivInpScalar(ctx, self, other, RoundModeNone)

- schema: "aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiDivScalar(ctx, out, self, other, RoundModeNone)

- schema: "div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div__scalar(self, other.item());
    }
  interface: diopiDivInp(ctx, self, other, RoundModeNone)

- schema: "aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_div_scalar_out(other, self.item(), out);
    }
  interface: diopiDiv(ctx, out, self, other, RoundModeNone)

- schema: "aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDivScalar(ctx, out, self, other, mode)

- schema: "aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div_scalar_mode_out(self, other.item(), rounding_mode, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_div_scalar_mode_out(other, self.item(), rounding_mode, out);
    }
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDiv(ctx, out, self, other, mode)

- schema: "mul.Scalar(Tensor self, Scalar other) -> Tensor"
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  interface: diopiMulScalar(ctx, out, self, other)

- schema: "mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiMulInpScalar(ctx, self, other)

- schema: "aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMulScalar(ctx, out, self, other)

- schema: "mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_mul__scalar(self, other.item());
    }
  interface: diopiMulInp(ctx, self, other)

- schema: "mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_mul_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_mul_scalar_out(other, self.item(), out);
    }
  interface: diopiMul(ctx, out, self, other)

- schema: "logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (out.numel() == 0) {
        auto shape = at::infer_size(self.sizes(), other.sizes());
        out.resize_(shape);
    }
  interface: diopiLogicalAnd(ctx, out, self, other);

- schema: "logical_and_(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  interface: diopiLogicalAndInp(ctx, self, other)

- schema: "logical_and(Tensor self, Tensor other) -> Tensor"
  custom_code_at_the_beginning: |
    auto shape = at::infer_size(self.sizes(), other.sizes());
    auto out = at::empty(shape, self.options().dtype(at::kBool));
  interface: diopiLogicalAnd(ctx, out, self, other);

- schema: "logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (out.numel() == 0) {
        auto shape = at::infer_size(self.sizes(), other.sizes());
        out.resize_(shape);
    }
  interface: diopiLogicalOr(ctx, out, self, other);

- schema: "logical_or_(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  interface: diopiLogicalOrInp(ctx, self, other)

- schema: "logical_or(Tensor self, Tensor other) -> Tensor"
  custom_code_at_the_beginning: |
    auto shape = at::infer_size(self.sizes(), other.sizes());
    auto out = at::empty(shape, self.options().dtype(at::kBool));
  interface: diopiLogicalOr(ctx, out, self, other);

- schema: "logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (out.numel() == 0) {
        out.resize_(self.sizes());
    }
  interface: diopiLogicalNot(ctx, out, self);

- schema: "logical_not_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiLogicalNotInp(ctx, self)

- schema: "logical_not(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty(self.sizes(), self.options().dtype(at::kBool));
  interface: diopiLogicalNot(ctx, out, self);

- schema: "aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  interface: diopiBatchNorm(ctx, out, save_mean, save_invstd, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);

- schema: "aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)"
  custom_code_at_the_beginning: |
    const int64_t dim_c = input.size(1);
    const auto input_shape = input.sizes();
    const int axis = input_shape.size();
    auto out0_MemoryFormat = at::MemoryFormat::Preserve;
    if(axis == 4 ){
        out0_MemoryFormat = $SUGGESTED_MEMORYFORMAT;
    }
    if(axis == 5){
        out0_MemoryFormat = $SUGGESTED_MEMORYFORMAT_3D;
    }
    auto out0 = at::empty_like(input, input.options(), out0_MemoryFormat);
    auto options = input.options().dtype(at::kFloat);
    at::Tensor out1, out2;
    if (!training) {
        // do not require save_mean/save_invstd when in test mode
        out1 = at::empty({0}, options);
        out2 = at::empty({0}, options);
    } else {
        out1 = at::empty({dim_c}, options);
        out2 = at::empty({dim_c}, options);
    }
  interface: diopiBatchNorm(ctx, out0, out1, out2, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);

- schema: "native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  custom_code_at_the_beginning: |
    int64_t dim_c = input.size(1);
    auto options = input.options().dtype(at::kFloat);
    at::Tensor out0 = at::empty_like(input, input.options(), $SUGGESTED_MEMORYFORMAT);
    at::Tensor out1 = at::empty({dim_c}, options);
    at::Tensor out2 = at::empty({dim_c}, options);
  interface: diopiBatchNormBackward(ctx, out0, out1, out2, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps)

- schema: "native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)"
  custom_code_at_the_beginning: |
    auto out0 = at::empty_like(input);
    auto options = input.options().dtype(at::kFloat);
    auto out1 = at::empty({N.expect_int(), group}, options);
    auto out2 = at::empty({N.expect_int(), group}, options);
  interface: diopiGroupNorm(ctx, out0, out1, out2, input, weight, bias, group, eps);

- schema: "native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  custom_code_at_the_beginning: |
    auto out0 = output_mask[0] ? at::empty_like(input) : at::Tensor();
    auto out1 = output_mask[1] ? at::empty_like(weight.value()) : at::Tensor();
    auto out2 = output_mask[2] ? at::empty_like(weight.value()) : at::Tensor();
  interface: diopiGroupNormBackward(ctx, out0, out1, out2, grad_out, input, weight, mean, rstd, group);

- schema: "native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor out, Tensor save_mean, Tensor save_invstd)"
  custom_code_at_the_beginning: |
    const auto input_shape = input.sizes();
    const int axis = input_shape.size() - normalized_shape.size();
    const int64_t M = c10::multiply_integers(input_shape.cbegin(), input_shape.cbegin() + axis);
    std::vector<int64_t> stats_shape(input_shape.size(), 1);
    std::copy(input_shape.begin(), input_shape.begin() + axis, stats_shape.begin());
    auto options = input.options();
    auto save_mean = at::empty(stats_shape, options);
    auto save_invstd = at::empty(stats_shape, options);
    auto out = at::empty_like(input);
  interface: diopiLayerNorm(ctx,  out,  save_mean,  save_invstd,  input,  weight,  bias, normalized_shape, eps);

- schema: "native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  custom_code_at_the_beginning: |
    auto options = grad_out.options();
    auto grad_input = output_mask[0] ? at::empty(input.sizes(), options) : at::Tensor();
    auto grad_weight = output_mask[1] ? at::empty(weight.value().sizes(), options) : at::Tensor();
    auto grad_bias = output_mask[2] ? at::empty(bias.value().sizes(), options) : at::Tensor();
  interface: diopiLayerNormBackward(ctx, grad_input, grad_weight, grad_bias, grad_out, input, weight,  bias, mean, rstd, normalized_shape);

- schema: "aten::native_layer_norm_backward.out(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  interface: diopiLayerNormBackward(ctx, grad_input, grad_weight, grad_bias, grad_out, input, weight,  bias, mean, rstd, normalized_shape);

- schema: "adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  #autocompare: disable # TODO: cpu impl not support half now
  interface: diopiAdaptiveAvgPool2d(ctx, out, self, output_size)

- schema: "_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor"
  custom_code_at_the_beginning: |
    TORCH_CHECK(output_size.size() == 2, __func__, ":", __FILE__, ":", __LINE__,
        " output_size should equal 2, size is ", output_size.size());
    auto out_tensor_size = self.sizes().vec();
    out_tensor_size[self.dim() - 2] = output_size[0].expect_int();
    out_tensor_size[self.dim() - 1] = output_size[1].expect_int();
    at::Tensor out = at::empty(out_tensor_size, self.options());
  interface: diopiAdaptiveAvgPool2d(ctx, out, self, output_size)

- schema: "_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiAdaptiveAvgPool2dBackward(ctx, out, grad_output, self);

- schema: "avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)"
  size_attr: [kernel_size, stride, padding]
  interface: "diopiAvgPool2d(ctx, out, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override.has_value() ? &divisor_override.value() : nullptr)"

- schema: aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
  size_attr: [kernel_size, stride, padding]
  interface: "diopiAvgPool2dBackward(ctx, grad_input, grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override.has_value() ? &divisor_override.value() : nullptr)"

- schema: "eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiEqScalar(ctx, out, self, other)

- schema: "eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_eq_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_eq_scalar_out(other, self.item(), out);
    }
  interface: diopiEq(ctx, out, self, other)

- schema: "eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiEqInpScalar(ctx, self, other)

- schema: "eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_eq__scalar(self, other.item());
    }
  interface: diopiEqInp(ctx, self, other)

- schema: "lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLtScalar(ctx, out, self, other)

- schema: "lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_lt_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_lt_scalar_out(other, self.item(), out);
    }
  interface: diopiLt(ctx, out, self, other)

- schema: "lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiLtInpScalar(ctx, self, other)

- schema: "lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_lt__scalar(self, other.item());
    }
  interface: diopiLtInp(ctx, self, other)

- schema: "ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiNeScalar(ctx, out, self, other)

- schema: "ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ne_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_ne_scalar_out(other, self.item(), out);
    }
  interface: diopiNe(ctx, out, self, other)

- schema: "ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiNeInpScalar(ctx, self, other)

- schema: "ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ne__scalar(self, other.item());
    }
  interface: diopiNeInp(ctx, self, other)

- schema: "ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiGeScalar(ctx, out, self, other)

- schema: "ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ge_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_ge_scalar_out(other, self.item(), out);
    }
  interface: diopiGe(ctx, out, self, other)

- schema: "ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiGeInpScalar(ctx, self, other)

- schema: "ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ge__scalar(self, other.item());
    }
  interface: diopiGeInp(ctx, self, other)

- schema: "gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiGtScalar(ctx, out, self, other)

- schema: "gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_gt_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_gt_scalar_out(other, self.item(), out);
    }
  interface: diopiGt(ctx, out, self, other)

- schema: "gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiGtInpScalar(ctx, self, other)

- schema: "gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_gt__scalar(self, other.item());
    }
  interface: diopiGtInp(ctx, self, other)

- schema: "le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLeScalar(ctx, out, self, other)

- schema: "le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_le_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_le_scalar_out(other, self.item(), out);
    }
  interface: diopiLe(ctx, out, self, other)

- schema: "le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiLeInpScalar(ctx, self, other)

- schema: "le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_le__scalar(self, other.item());
    }
  interface: diopiLeInp(ctx, self, other)

- schema: "relu_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiReluInp(ctx, self)

- schema: "relu(Tensor self) -> Tensor"
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  interface: diopiRelu(ctx, out, self)

- schema: "randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)"
  autocompare: disable
  custom_code_at_the_beginning: |
    diopiGeneratorHandle_t generatorDiopiGenerator = toDiopiGeneratorHandle(getDefaultDIPUGenerator());
  interface: diopiRandperm(ctx, out, n, generatorDiopiGenerator)

- schema: "randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)"
  autocompare: disable
  interface: diopiRandperm(ctx, out, n, generator)

- schema: "aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const auto self_dtype = at::native::to(self, dtype);
    ::diopiConstTensorHandle_t self_dtype_diopi = dipu::diopi_helper::toDiopiTensorHandle(self_dtype);
    if (out.numel() == 0) {
      std::vector<int64_t> output_shape = infer_reduce_op_shape(self.sizes(), dim.value_or(std::vector<int64_t>()), keepdim);
      out = at::empty(output_shape, self.options());
      }
    ::diopiSize_t diopi_size = toDiopiSize(dim);
  interface: diopiSum(ctx, out, self_dtype_diopi, diopi_size)

- schema: "addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
  interface: diopiAddmm(&context, out, self, mat1, mat2, beta, alpha)

- schema: "cross_entropy_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor"
  register_op: False
  custom_code_at_the_beginning: |
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor out = at::empty_like(self);
  interface: diopiCrossEntropyLossBackward(ctx, out, grad_output, self, target, weight, reductionDiopi, ignore_index.expect_int(), label_smoothing)

- schema: "cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor"
  register_op: False
  custom_code_at_the_beginning: |
    const int64_t ignore_index_int = ignore_index.expect_int();
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor out;
    auto options = self.options();
    if (reductionDiopi == ReductionNone) {
      out = at::empty(target.sizes(), options);
    } else {
      out = at::empty({}, options);
    }
  interface: diopiCrossEntropyLoss(ctx, out, self, target, weight, reductionDiopi, ignore_index_int, label_smoothing)
  backward_schema: "cross_entropy_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor"
  saved_data: [reduction, ignore_index, label_smoothing, weight, self, target]
  cal_grad_code: |
    auto grad_output = grad_outputs.at(0);
    auto reduction = reduction_.toInt();
    auto ignore_index = ignore_index_.toInt();
    auto label_smoothing = label_smoothing_.toDouble();
    auto weight = weight_.toOptional<at::Tensor>();
    auto self = self_.toTensor();
    auto target = target_.toTensor();
  backward_return_code: |
    std::vector<at::Tensor> outputs(6);
    outputs[0] = result;
    return outputs;

- schema: "convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor"
  custom_fallback: True
  size_attr: [stride, padding, dilation]
  custom_code_at_the_beginning: |
    int64_t batch_size = input.size(0);
    int64_t height = input.size(2);
    int64_t width = input.size(3);
    int64_t out_channel = weight.size(0);
    auto kernel_size = weight.sizes().slice(2);
    int64_t out_height = (height + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1;
    int64_t out_width = (width + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1;
    c10::SmallVector<int64_t, 4> output_size = {batch_size, out_channel, out_height, out_width};
    at::Tensor out = at::empty(output_size, input.options(),$SUGGESTED_MEMORYFORMAT);
  interface: diopiConvolution2d(&context, out, input, weight, bias, stride, padding, dilation, groups)

- schema: "convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  custom_fallback: True
  size_attr: [stride, padding, dilation, bias_sizes]
  custom_code_at_the_beginning: |
    at::Tensor grad_input;
    at::Tensor grad_weight;
    at::Tensor grad_bias;
    std::vector<int64_t> bias_sizes;
    if (output_mask[0]) {
      grad_input = at::empty(input.sizes(), input.options(), $SUGGESTED_MEMORYFORMAT);
    }
    if (output_mask[1]) {
      grad_weight = at::empty(weight.sizes(), weight.options().dtype(at::kFloat));
    }
    if (output_mask[2]) {
      bias_sizes.push_back(grad_output.size(1));
      grad_bias = at::empty(bias_sizes, grad_output.options());
    }
  custom_code_before_call_diopi: |
    ::diopiSize_t* bias_sizes_ptr = output_mask[2] ? &bias_sizesDiopiSize : nullptr;
  interface: diopiConvolution2dBackward(ctx, grad_input, grad_weight, grad_bias, grad_output, input, weight, bias_sizes_ptr, stride, padding, dilation, groups);

- schema: "convolution_transpose_backward(Tensor grad_output, Tensor input, Tensor weight, int[] bias_sizes, int[] stride, int[] padding, int[] dilation, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  register_op: False
  size_attr: [stride, padding, dilation, bias_sizes, output_padding]
  custom_code_at_the_beginning: |
    at::Tensor grad_input;
    at::Tensor grad_weight;
    at::Tensor grad_bias;
    grad_input = at::empty(input.sizes(), input.options(), $SUGGESTED_MEMORYFORMAT);
    grad_weight = at::empty(weight.sizes(), weight.options().dtype(at::kFloat));
    if (output_mask[2]) {
        grad_bias = at::empty({grad_output.size(1)}, grad_output.options());
    }
  custom_code_before_call_diopi: |
    ::diopiSize_t* bias_sizes_ptr = output_mask[2] ? &bias_sizesDiopiSize : nullptr;
  interface: diopiConvTranspose2dBackward(ctx, grad_input, grad_weight, grad_bias, grad_output, input, weight, bias_sizes_ptr, stride, padding, dilation, output_padding, groups);

- schema: "conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor"
  autograd: True
  size_attr: [stride, padding, output_padding, dilation]
  custom_code_at_the_beginning: |
    const int64_t n = input.size(0);
    const int64_t w_in = input.size(-1);
    const int64_t h_in = input.size(-2);
    const int64_t kernel_width = weight.size(-1);
    const int64_t kernel_height = weight.size(-2);
    const int64_t h_out = (h_in - 1) * stride[0] - 2 * padding[0] + (dilation[0] * (kernel_height - 1) + 1) + output_padding[0];
    const int64_t w_out = (w_in - 1) * stride[1] - 2 * padding[1] + (dilation[1] * (kernel_width - 1) + 1) + output_padding[1];
    const int64_t c_out = weight.size(1) * groups;
    auto output_shape =  input.sizes().size() == 3 ? std::vector<int64_t>{c_out, h_out, w_out} : std::vector<int64_t>{n, c_out, h_out, w_out};
    auto out = at::empty(output_shape, input.options(), $SUGGESTED_MEMORYFORMAT);
  interface: diopiConvTranspose2d(ctx, out, input, weight, bias, stride, padding, output_padding, groups, dilation)
  forward_process_code: |
    bool bias_has_value = (bias.has_value() == true) ? bias.value().requires_grad() : false;
  saved_data:
    [
      stride,
      padding,
      dilation,
      groups,
      bias_has_value,
      input,
      weight,
      output_padding,
    ]
  cal_grad_code: |
    auto grad_output = grad_outputs.at(0);
    auto input = input_.toTensor();
    auto weight = weight_.toTensor();
    auto padding = padding_.toIntVector();
    auto stride = stride_.toIntVector();
    auto dilation = dilation_.toIntVector();
    auto output_padding = output_padding_.toIntVector();
    bool bias_has_value = bias_has_value_.toBool();
    auto groups = groups_.toInt();
    std::vector<int64_t> bias_sizes;
    if (bias_has_value) {
      bias_sizes.push_back(grad_output.size(1));
    }
    std::array<bool, 3> output_mask;
    output_mask[0] = input.requires_grad();
    output_mask[1] = weight.requires_grad();
    output_mask[2] = bias_has_value;
  backward_schema: "convolution_transpose_backward(Tensor grad_output, Tensor input, Tensor weight, int[] bias_sizes, int[] stride, int[] padding, int[] dilation, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  backward_return_code: |
    std::vector<at::Tensor>  outputs = {
          std::get<0>(result), std::get<1>(result), std::get<2>(result),
          at::Tensor(), at::Tensor(), at::Tensor(), at::Tensor(), at::Tensor()};
    return outputs;

- schema: "native_dropout(Tensor input, float p, bool? train) -> (Tensor, Tensor)"
  custom_code_at_the_beginning: |
    at::Tensor out0 = at::empty_like(input);
    at::Tensor out1;
    bool train_ = train.value_or(false);
    if (train_) {
      out1 = at::empty(input.sizes(), input.options().dtype(at::kByte));;
    }
    diopiGeneratorHandle_t generatorDiopiGenerator = toDiopiGeneratorHandle(getDefaultDIPUGenerator());
  interface: diopiDropout(ctx, out0, out1, input, p, train_, generatorDiopiGenerator)

- schema: "native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -> Tensor"
  custom_code_at_the_beginning: |
    at::Tensor out = grad_output * mask * scale;
    return out;
  interface: diopiMul(ctx, out, out, out)

- schema: "bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)"
  autocompare: disable
  interface: diopiBernoulliScalar(ctx, self, p, generatorDiopiGenerator);

- schema: "log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLog(ctx, out, self)

- schema: "log_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiLogInp(ctx, self)

- schema: "log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLog2(ctx, out, self)

- schema: "log2_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiLog2Inp(ctx, self)

- schema: "abs(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiAbs(ctx, out, self)

- schema: "abs_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiAbsInp(ctx, self)

- schema: "abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAbs(ctx, out, self)

- schema: "neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiNeg(ctx, out, self)

- schema: "neg_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiNegInp(ctx, self)

- schema: "sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSqrt(ctx, out, self)

- schema: "sqrt_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiSqrtInp(ctx, self)

- schema: "all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAll(ctx, out, self, &dim)

- schema: "all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAll(ctx, out, self, nullptr)

- schema: "any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAny(ctx, out, self, nullptr)

- schema: "any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAny(ctx, out, self, &dim)

- schema: "topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  interface: diopiTopk(ctx, values, indices, self, k, dim, largest, sorted)

- schema: "topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)"
  custom_code_at_the_beginning: |
    std::vector<int64_t> output_size(self.sizes().begin(), self.sizes().end());
    dim = dim < 0 ? (dim + output_size.size()) : dim;
    output_size[dim] = k;
    auto values = at::empty(output_size, self.options());
    auto indices = at::empty(output_size, self.options().dtype(at::kLong));
  interface: diopiTopk(ctx, values, indices, self, k, dim, largest, sorted)

- schema: "mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const auto self_dtype = at::native::to(self, dtype);
    ::diopiConstTensorHandle_t self_dtype_diopi = dipu::diopi_helper::toDiopiTensorHandle(self_dtype);
    ::diopiSize_t diopi_size = toDiopiSize(dim);
  interface: diopiMean(ctx, out, self_dtype_diopi, diopi_size);

- schema: "std.correction(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False) -> Tensor"
  custom_code_at_the_beginning: |
    std::vector<int64_t> output_shape = infer_reduce_op_shape(self.sizes(), dim.value_or(std::vector<int64_t>()), keepdim);
    auto out = at::empty(output_shape, self.options());
    bool unbiased = correction.value_or(1) == 1;
    ::diopiSize_t diopi_size = toDiopiSize(dim);
  interface: diopiStd(ctx, out, self, diopi_size, unbiased);

- schema: "std.correction_out(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    ::diopiSize_t diopi_size = toDiopiSize(dim);
    bool unbiased = correction.value_or(1) == 1;
  interface: diopiStd(ctx, out, self, diopi_size, unbiased);

- schema: "linear_backward(Tensor input, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  device: [all, -cuda]
  custom_fallback: True
  custom_code_at_the_beginning: |
    at::Tensor grad_input, grad_weight, grad_bias;
    if (output_mask[0]) {
      grad_input = at::empty(input.sizes(), grad_output.options());
    }
    if (output_mask[1]) {
      grad_weight = at::empty(weight.sizes(), grad_output.options());
    }
    if (output_mask[2]) {
      grad_bias = at::empty({grad_output.size(-1)}, grad_output.options());
    }
  interface: diopiLinearBackward(ctx, grad_input, grad_weight, grad_bias, grad_output, input, weight)

- schema: "linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
  device: [all, -cuda]
  custom_code_at_the_beginning: |
    std::vector<int64_t> output_size(input.sizes().begin(), input.sizes().end());
    output_size.back() = weight.sizes()[0];
    auto out = at::empty(output_size, input.options());
  interface: diopiLinear(ctx, out, input, weight, bias)

- schema: "_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLogSoftmaxBackward(ctx, out, grad_output, output, dim)

- schema: "_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLogSoftmax(ctx, out, self, dim)

- schema: "aten::log_softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const auto self_dtype = at::native::to(self, dtype);
    ::diopiConstTensorHandle_t self_dtype_diopi = dipu::diopi_helper::toDiopiTensorHandle(self_dtype);
  interface: diopiLogSoftmax(ctx, out, self_dtype_diopi, dim)

- schema: "max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))"
  size_attr: [kernel_size, stride, padding, dilation]
  interface: diopiMaxPool2dWithIndices(&context, out, indices, self, kernel_size, stride, padding, dilation, ceil_mode)

- schema: max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
  size_attr: [kernel_size, stride, padding, dilation]
  interface: diopiMaxPool2dBackward(ctx, grad_input, grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices)

- schema: "max_pool2d_backward(Tensor grad_output, Tensor input, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor? indices=None) -> Tensor"
  device: [topsrider]
  register_op: False
  size_attr: [kernel_size, stride, padding, dilation]
  custom_code_at_the_beginning: |
    auto out = at::empty(input.sizes(), grad_output.options());
  interface: diopiMaxPool2dBackward(ctx, out, grad_output, input, kernel_size, stride, padding, dilation, ceil_mode, indices)

- schema: "max_pool2d(Tensor input, int[2] kernel_size=1, int[2] stride=1, int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor"
  device: [topsrider]
  size_attr: [kernel_size, stride, padding, dilation]
  custom_code_at_the_beginning: |
    int64_t batch_size = input.size(0);
    int64_t channel = input.size(1);
    int64_t height = input.size(2);
    int64_t width = input.size(3);
    int64_t out_height = std::floor((height + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1);
    int64_t out_width = std::floor((width + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1);
    c10::SmallVector<int64_t, 4> output_size = {batch_size, channel, out_height, out_width};
    at::Tensor out = at::empty(output_size, input.options());
  interface: diopiMaxPool2d(&context, out, input, kernel_size, stride, padding, dilation, ceil_mode)
  autograd: True
  saved_data: [kernel_size, stride, padding, dilation, input, ceil_mode]
  backward_schema: "max_pool2d_backward(Tensor grad_output, Tensor input, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor? indices=None) -> Tensor"
  cal_grad_code: |
    auto input = input_.toTensor();
    auto grad_output = grad_outputs.at(0);
    auto kernel_size = kernel_size_.toIntVector();
    auto padding = padding_.toIntVector();
    auto stride = stride_.toIntVector();
    auto dilation = dilation_.toIntVector();
    bool ceil_mode = ceil_mode_.toBool();
    at::Tensor indices;
  backward_return_code: |
    std::vector<at::Tensor> outputs(6);
    outputs[0] = result;
    return outputs;

- schema: nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiNLLLoss(ctx, out, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int())

- schema: nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
  interface: diopiNLLLoss(ctx, output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int())

- schema: nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
  interface: diopiNLLLoss(ctx, output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int())

- schema: nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
  interface: diopiNLLLoss(ctx, output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int())
  custom_code_at_the_beginning: |
    at::Tensor output;
    at::Tensor total_weight = at::scalar_tensor(target.numel(), self.options());

    if (reduction != 0) {
        output = torch::tensor(0.0, self.options());
    } else {
        output = at::empty(target.sizes(), self.options());
    }

- schema: nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
  interface: diopiNLLLossBackward(&context, grad_input, grad_output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int());

- schema: nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
  interface: diopiNLLLossBackward(&context, grad_input, grad_output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int());

- schema: nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor grad_input
  interface: diopiNLLLossBackward(&context, grad_input, grad_output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int());
  custom_code_at_the_beginning: |
    auto grad_input = at::empty(self.sizes(), self.options());

- schema: "threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiThresholdBackward(ctx, grad_input, grad_output, self, &threshold)

- schema: "transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)"
  register_op: False
  custom_code_at_the_beginning: |
    std::vector<int64_t> output_size(self.sizes().cbegin(), self.sizes().cend());
    std::vector<int64_t> output_stride(self.strides().cbegin(), self.strides().cend());
    std::swap(output_size[dim0], output_size[dim1]);
    std::swap(output_stride[dim0], output_stride[dim1]);
    self.sizes() = output_size;
    self.strides() = output_stride;
    return self;
  interface: diopiTranspose(ctx, self, self, dim0, dim1)

- schema: "bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseAnd(ctx, out, self, other)

- schema: "bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  interface: diopiBitwiseAndInp(ctx, self, other)

- schema: "bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseAndScalar(ctx, out, self, other)

- schema: "bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiBitwiseAndInpScalar(ctx, self, other)

- schema: "bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseOr(ctx, out, self, other)

- schema: "bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  interface: diopiBitwiseOrInp(ctx, self, other)

- schema: "bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseOrScalar(ctx, out, self, other)

- schema: "bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiBitwiseOrInpScalar(ctx, self, other)

- schema: "bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseNot(ctx, out, self)

- schema: "bitwise_not_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiBitwiseNotInp(ctx, self)

- schema: "stack(Tensor[] tensors, int dim=0) -> Tensor"
  custom_code_at_the_beginning: |
    dim += dim < 0 ? tensors[0].sizes().size()+1 : 0;
    auto num_tensors = tensors.size();
    auto shape = tensors[0].sizes();
    std::vector<int64_t> tmp;
    for (int i = 0; i < dim; i++) {
        tmp.push_back(shape[i]);
    }
    tmp.push_back(num_tensors);
    for (int i = dim; i < shape.size(); i++) {
        tmp.push_back(shape[i]);
    }
    const std::vector<int64_t>& const_tmp = tmp;
    shape = at::ArrayRef<int64_t>(const_tmp);
    auto out = at::empty({shape}, tensors[0].options());

    std::vector<diopiConstTensorHandle_t> diopiTensorHandles(tensors.size());
    for (size_t i = 0; i < tensors.size(); ++i) {
      diopiTensorHandles[i] = dipu::diopi_helper::toDiopiTensorHandle(tensors.at(i));
    }
  interface: diopiStack(ctx, out, diopiTensorHandles.data(), num_tensors, dim)

- schema: "stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    dim += dim < 0 ? tensors[0].sizes().size() : 0;
    std::vector<diopiConstTensorHandle_t> diopiTensorHandles(tensors.size());
    for (size_t i = 0; i < tensors.size(); ++i) {
      diopiTensorHandles[i] = dipu::diopi_helper::toDiopiTensorHandle(tensors.at(i));
    }
  interface: diopiStack(ctx, out, diopiTensorHandles.data(), tensors.size(), dim)

- schema: "sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)"
  custom_code_at_the_beginning: |
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
    auto values = at::empty(self.sizes(), self.options());
    auto indices = at::empty(self.sizes(), self.options().dtype(at::kLong));
  interface: diopiSort(ctx, values, indices, self, dim_, descending, nullptr)

- schema: "sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  custom_code_at_the_beginning: |
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
  interface: diopiSort(ctx, values, indices, self, dim_, descending, nullptr)

- schema: "sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  custom_code_at_the_beginning: |
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
    bool stable_ = stable.has_value() ? stable.value() : false;
    const bool *p = &stable_;
  interface: diopiSort(ctx, values, indices, self, dim_, descending, p)

- schema: "rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiRsqrt(ctx, out, self)

- schema: "uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)"
  autocompare: disable
  interface: diopiUniformInp(ctx, self, from, to, generator)

- schema: "tril(Tensor self, int diagonal=0) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiTril(ctx, out, self, diagonal)

- schema: "tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiTril(ctx, out, self, diagonal)

- schema: "multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
    if (self.dim() == 2){
      out = at::empty({self.size(0), num_samples}, self.options().dtype(at::kLong));
    }
    else if (self.dim() == 1) {
      out = at::empty({num_samples,}, self.options().dtype(at::kLong));
    }
  interface: diopiMultinomial(ctx, out, self, num_samples, replacement, generator)

- schema: "multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMultinomial(ctx, out, self, num_samples, replacement, generator)

- schema: "roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
    ::diopiSize_t diopi_shifts = toDiopiSize(shifts);
    ::diopiSize_t diopi_dims = toDiopiSize(dims);
  interface: diopiRoll(ctx, out, self, diopi_shifts, diopi_dims)

- schema: "leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiLeakyRelu(ctx, out, self, negative_slope)

- schema: "leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)"
  interface: diopiLeakyReluInp(ctx, self, negative_slope)

- schema: "leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLeakyRelu(ctx, out, self, negative_slope)

- schema: "leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiLeakyReluBackward(ctx, grad_input, grad_output, self, negative_slope, self_is_result)

- schema: "mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor"
  custom_code_at_the_beginning: |
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor out;
    auto options = self.options();
    if (reductionDiopi == ReductionNone) {
      out = at::empty(self.sizes(), options);
    } else {
      out = at::empty({}, options);
    }
  interface: diopiMSELoss(ctx, out, self, target, reductionDiopi)

- schema: "mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
  interface: diopiMSELoss(ctx, out, self, target, reductionDiopi)

- schema: "mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor grad_input"
  custom_code_at_the_beginning: |
    auto grad_input = at::empty(self.sizes(), grad_output.options());
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
  interface: diopiMSELossBackward(ctx, grad_input, grad_output, self, target, reductionDiopi)

- schema: "clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)"
  interface: diopiClampInpScalar(ctx, self, min, max)

- schema: "clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiClampScalar(ctx, out, self, min, max)

- schema: "clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)"
  interface: diopiClampInp(ctx, self, min, max)

- schema: "clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiClamp(ctx, out, self, min, max)

- schema: "random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)"
  autocompare: disable
  interface: diopiRandomInp(ctx, self, 0, nullptr, generator)

- schema: "random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)"
  autocompare: disable
  interface: diopiRandomInp(ctx, self, 0, &to, generator)

- schema: "random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)"
  autocompare: disable
  interface: "diopiRandomInp(ctx, self, from, to.has_value() ? &to.value() : nullptr, generator)"

- schema: "nonzero(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    at::Tensor out;
    diopiTensorHandle_t out_ptr = nullptr;
  interface: diopiNonzero(ctx, &out_ptr, self);
  custom_code_before_return: |
    dipu::getCurrentDIPUStream().synchronize();
    out = *reinterpret_cast<at::Tensor*>(out_ptr);
- schema: _softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiSoftmax(ctx, out, self, dim);

- schema: "_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiSoftmaxBackward(ctx, grad_input, grad_output, output, dim);

- schema: pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiPowTensor(ctx, out, self, exponent);

- schema: pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiPow(ctx, out, self, exponent);

- schema: pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)
  interface: diopiPowInpTensor(ctx, self, exponent);

- schema: pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)
  interface: diopiPowInp(ctx, self, exponent);

- schema: pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiPowScalar(ctx, out, self, exponent)

- schema: prod(Tensor self, *, ScalarType? dtype=None) -> Tensor
  custom_code_at_the_beginning: |
    const auto self_dtype = at::native::to(self, dtype);
    auto out = at::empty({}, self_dtype.options());
    ::diopiConstTensorHandle_t self_dtype_diopi = dipu::diopi_helper::toDiopiTensorHandle(self_dtype);
  interface: diopiProd(ctx, out, self_dtype_diopi, nullptr)

- schema: prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
  custom_code_at_the_beginning: |
    const auto self_dtype = at::native::to(self, dtype);
    ::diopiConstTensorHandle_t self_dtype_diopi = dipu::diopi_helper::toDiopiTensorHandle(self_dtype);
  interface: diopiProd(ctx, out, self_dtype_diopi, &dim)

- schema: repeat(Tensor self, SymInt[] repeats) -> Tensor
  autocompare: disable
  custom_code_at_the_beginning: |
    std::vector<int64_t> output_size(repeats.size());
    for (int i = 0;i< repeats.size();++i) {
      output_size[i] = repeats.at(i).expect_int();
    }

    const auto& self_sizes = self.sizes();
    for (int i = self_sizes.size() - 1, j = output_size.size() - 1;i >= 0;i--, j--) {
      output_size[j] *= self_sizes.at(i);
    }

    at::Tensor out = at::empty(output_size, self.options());
  interface: diopiRepeat(ctx, out, self, repeats)

- schema: rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
    return dipu_sub_out(other, self, alpha, out);
  interface: diopiSub(ctx, out, other, self, alpha)

- schema: "unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor out, Tensor indices, Tensor counts)"
  custom_code_at_the_beginning: |
    at::Tensor out, counts, indices;
    if (return_inverse) {
      const auto ndims = self.sizes().size();
      dim += (dim < 0 ? ndims : 0);
      indices = at::empty({self.sizes().at(dim)}, self.options().dtype(at::kLong));
    }
    diopiTensorHandle_t out_ptr = nullptr;
    diopiTensorHandle_t counts_ptr = nullptr;
  interface: diopiUnique(ctx, &out_ptr, self, &dim, sorted, return_counts, indices, &counts_ptr);
  custom_code_before_return: |
    dipu::getCurrentDIPUStream().synchronize();
    out = *reinterpret_cast<at::Tensor*>(out_ptr);
    if (return_counts) {
      counts = *reinterpret_cast<at::Tensor*>(counts_ptr);
    }

- schema: "_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor out, Tensor indices, Tensor counts)"
  custom_code_at_the_beginning: |
    at::Tensor out, counts, indices;
    if (return_inverse) {
      indices = at::empty(self.sizes(), self.options().dtype(at::kLong));
    }
    diopiTensorHandle_t out_ptr = nullptr;
    diopiTensorHandle_t counts_ptr = nullptr;
  interface: diopiUnique(ctx, &out_ptr, self, nullptr, sorted, return_counts, indices, &counts_ptr);
  custom_code_before_return: |
    dipu::getCurrentDIPUStream().synchronize();
    out = *reinterpret_cast<at::Tensor*>(out_ptr);
    if (return_counts) {
      counts = *reinterpret_cast<at::Tensor*>(counts_ptr);
    }

- schema: "aten::cat.out(const at::ITensorListRef & tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    std::vector<diopiConstTensorHandle_t> diopiTensorHandles(tensors.size(), nullptr);
    std::transform(tensors.begin(), tensors.end(), diopiTensorHandles.begin(), [](const at::Tensor& tensor){
        return dipu::diopi_helper::toDiopiTensorHandle(tensor);
    });
  interface: diopiCat(ctx, out, diopiTensorHandles.data(), tensors.size(), dim);

- schema: "masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiMaskedFill(ctx, out, self, mask, value)

- schema: "masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)"
  interface: diopiMaskedFillInp(ctx, self, mask, value)

- schema: "masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiMaskedFillScalar(ctx, out, self, mask, value)

- schema: "masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)"
  interface: diopiMaskedFillInpScalar(ctx, self, mask, value)

- schema: "min(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty({}, self.options());
  interface: diopiMinAll(ctx, out, self)

- schema: "min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) min, Tensor(b!) min_indices)"
  custom_code_at_the_beginning: |
    dim += ((dim >= 0) ? 0 : self.sizes().size());
  interface: diopiMin(ctx, min, min_indices, self, dim)

- schema: "max(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty({}, self.options());
  interface: diopiMaxAll(ctx, out, self)

- schema: "maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  no_device_check_args: [self, other]
  ins: [selfTemp, otherTemp]
  custom_code_at_the_beginning: |
    auto selfTemp = (self.numel() == 1 && self.is_cpu()) ? self.to(other.device()) : self;
    auto otherTemp = (other.numel() == 1 && other.is_cpu()) ? other.to(self.device()) : other;
  interface: diopiMaximum(ctx, out, selfTemp, otherTemp)

- schema: "max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_indices) -> (Tensor(a!) max, Tensor(b!) max_indices)"
  custom_code_at_the_beginning: |
    dim += ((dim >= 0) ? 0 : self.sizes().size());
    if (max_indices.numel() <= 0) {
      auto output_size = self.sizes().vec();
      if (keepdim) {
        output_size[dim] = 1;
      } else {
        output_size.erase(output_size.begin() + dim);
      }
      max_indices.resize_(output_size);
    }
  interface: diopiMax(ctx, max, max_indices, self, dim)

- schema: "addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAddcdiv(ctx, out, self,tensor1,tensor2,value)

- schema: "addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)"
  interface: diopiAddcdivInp(ctx,self , tensor1, tensor2, value)

- schema: "addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAddcmul(ctx, out, self, tensor1, tensor2, value)

- schema: "addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)"
  interface: diopiAddcmulInp(ctx,self,tensor1, tensor2, value)

- schema: "exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiExp(ctx, out, self)

- schema: "exp_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiExpInp(ctx, self)

- schema: "tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiTanh(ctx, out,self)

- schema: "tanh_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiTanhInp(ctx,self)

- schema: "argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const int64_t* ptr = dim.has_value() ? (&dim.value()): nullptr;
  interface: diopiArgmax(ctx, out, self, ptr, keepdim)

- schema: "masked_select(Tensor self, Tensor mask) -> Tensor"
  custom_code_at_the_beginning: |
    at::Tensor out;
    diopiTensorHandle_t out_ptr = nullptr;
  interface: diopiMaskedSelect(ctx, &out_ptr, self, mask);
  custom_code_before_return: |
    dipu::getCurrentDIPUStream().synchronize();
    out = *reinterpret_cast<at::Tensor*>(out_ptr);

- schema: "masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMaskedSelect(ctx, &out, self, mask);

- schema: "baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    double betaDouble = beta.toDouble();
    double alphaDouble = alpha.toDouble();
  interface: diopiBaddbmm(ctx, out, self, batch1, batch2, betaDouble, alphaDouble)

- schema: "baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    double betaDouble = beta.toDouble();
    double alphaDouble = alpha.toDouble();
  interface: diopiBaddbmmInp(ctx, self, batch1, batch2, betaDouble, alphaDouble)

- schema: "floor_divide(Tensor self, Tensor other) -> Tensor"
  no_device_check_args: [other]
  custom_code_at_the_beginning: |
    at::Tensor out = at::empty_like(self);
    out = dipu_div_out(self,other,out);
  interface: diopiFloorInp(ctx,out)

- schema: "where.self(Tensor condition, Tensor self, Tensor other) -> Tensor"
  custom_code_at_the_beginning: |
    auto shape = at::infer_size(condition.sizes(), self.sizes());
    shape = at::infer_size(shape, other.sizes());
    auto out = at::empty(shape, self.options());
  interface: diopiWhere(ctx, out, condition,self, other)

- schema: "gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)"
  interface: diopiGelu(ctx, out, self, approximate.data())

- schema: "gelu_backward.grad_input(Tensor grad_output, Tensor self, *, str approximate='none', Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiGeluBackward(ctx, grad_input, grad_output, self, approximate.data())

- schema: "hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor"
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  custom_code_before_call_diopi: |
    min_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(min_val, at::kDouble);
    max_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(max_val, at::kDouble);

  interface: diopiHardtanh(ctx, out, self, min_val, max_val)

- schema: "hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)"
  custom_code_before_call_diopi: |
    min_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(min_val, at::kDouble);
    max_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(max_val, at::kDouble);
  interface: diopiHardtanhInp(ctx, self, min_val, max_val)

- schema: "hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_before_call_diopi: |
    min_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(min_val, at::kDouble);
    max_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(max_val, at::kDouble);

  interface: diopiHardtanh(ctx, out, self, min_val, max_val)

- schema: "hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiHardtanhBackward(ctx, grad_input, grad_output, self, min_val, max_val)

- schema: "tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiTanhBackward(ctx, grad_input, grad_output, output)

- schema: "hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor grad_input"
  custom_code_at_the_beginning: |
    auto grad_input = at::empty(self.sizes(), grad_output.options());
  interface: diopiHardtanhBackward(ctx, grad_input, grad_output, self, min_val, max_val)

- schema: "upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  size_attr: [size]
  custom_code_at_the_beginning: |
    std::vector<int64_t> size(2);
  custom_code_before_call_diopi: |
    if (output_size.size() > 0) {
      std::copy(output_sizeVector.begin(), output_sizeVector.end(), size.begin());
    } else {
      size[0] = std::floor(self.size(-2) * scales_h.value_or(1.0));
      size[1] = std::floor(self.size(-1) * scales_w.value_or(1.0));
    }
  interface: diopiUpsampleNearest(ctx, out, self, size);

- schema: "upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor"
  size_attr: [size]
  custom_code_at_the_beginning: |
    std::vector<int64_t> size(2);
    if (output_size.size() > 0) {
      std::vector<int64_t> tmpVector(output_size.size());
      auto symIntToInt = [](const c10::SymInt& t)-> int64_t {return t.expect_int();};
      std::transform(output_size.cbegin(), output_size.cend(), tmpVector.begin(), symIntToInt);
      std::copy(tmpVector.begin(), tmpVector.end(), size.begin());
    } else {
      size[0] = std::floor(self.size(-2) * scales_h.value_or(1.0));
      size[1] = std::floor(self.size(-1) * scales_w.value_or(1.0));
    }
    auto out = at::empty({self.size(0),self.size(1),size[0],size[1]},self.options(),$SUGGESTED_MEMORYFORMAT);
  interface: diopiUpsampleNearest(ctx, out, self, size);

- schema: "upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  size_attr: [size]
  custom_code_at_the_beginning: |
    std::vector<int64_t> size(2);
  custom_code_before_call_diopi: |
    if (output_size.size() > 0) {
      std::copy(output_sizeVector.begin(), output_sizeVector.end(), size.begin());
    } else {
      size[0] = std::floor(self.size(-2) * scales_h.value_or(1.0));
      size[1] = std::floor(self.size(-1) * scales_w.value_or(1.0));
    }
    const char* mode = "bilinear";
  interface: diopiUpsampleLinear(ctx, out, self, size, align_corners, mode);

- schema: "upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  size_attr: [size]
  custom_code_at_the_beginning: |
    std::vector<int64_t> size(2);
    if (output_size.size() > 0) {
      std::vector<int64_t> tmpVector(output_size.size());
      auto symIntToInt = [](const c10::SymInt& t)-> int64_t {return t.expect_int();};
      std::transform(output_size.cbegin(), output_size.cend(), tmpVector.begin(), symIntToInt);
      std::copy(tmpVector.begin(), tmpVector.end(), size.begin());
    } else {
      size[0] = std::floor(self.size(-2) * scales_h.value_or(1.0));
      size[1] = std::floor(self.size(-1) * scales_w.value_or(1.0));
    }
    auto out = at::empty({self.size(0),self.size(1),size[0],size[1]},self.options(),$SUGGESTED_MEMORYFORMAT);
    const char* mode = "bilinear";
  interface: diopiUpsampleLinear(ctx, out, self, size, align_corners, mode);

- schema: "upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  size_attr: [size]
  custom_code_at_the_beginning: |
    std::vector<int64_t> size(2);
  custom_code_before_call_diopi: |
    if (output_size.size() > 0) {
      std::copy(output_sizeVector.begin(), output_sizeVector.end(), size.begin());
    } else {
      size[0] = std::floor((*(input_sizeVector.rbegin() + 1)) * scales_h.value_or(1.0));
      size[1] = std::floor((*(input_sizeVector.rbegin())) * scales_w.value_or(1.0));
    }
  interface: diopiUpsampleNearestBackward(ctx, grad_input, grad_output, size, input_size)

- schema: "upsample_nearest2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor grad_input"
  size_attr: [size]
  custom_code_at_the_beginning: |
    std::vector<int64_t> size(2);
    auto symInt2Int = [](const c10::SymInt& t)-> int64_t {return t.expect_int();};
    std::vector<int64_t> grad_input_shape(input_size.size());
    std::transform(input_size.cbegin(), input_size.cend(), grad_input_shape.begin(), symInt2Int);
    auto grad_input = at::empty(grad_input_shape,grad_output.options(),$SUGGESTED_MEMORYFORMAT);
  custom_code_before_call_diopi: |
    if (output_size.size() > 0) {
      std::copy(output_sizeVector.begin(), output_sizeVector.end(), size.begin());
    } else {
      size[0] = std::floor((*(input_sizeVector.rbegin() + 1)) * scales_h.value_or(1.0));
      size[1] = std::floor((*(input_sizeVector.rbegin())) * scales_w.value_or(1.0));
    }
  interface: diopiUpsampleNearestBackward(ctx, grad_input, grad_output, size, input_size)

- schema: "upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  size_attr: [size]
  custom_code_at_the_beginning: |
    std::vector<int64_t> size(2);
  custom_code_before_call_diopi: |
    if (output_size.size() > 0) {
      std::copy(output_sizeVector.begin(), output_sizeVector.end(), size.begin());
    } else {
      size[0] = std::floor((*(input_sizeVector.rbegin() + 1)) * scales_h.value_or(1.0));
      size[1] = std::floor((*(input_sizeVector.rbegin())) * scales_w.value_or(1.0));
    }
    const char* mode = "bilinear";
  interface: diopiUpsampleLinearBackward(ctx, grad_input, grad_output, size, input_size, align_corners, mode)

- schema: "upsample_bilinear2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor grad_input"
  size_attr: [size]
  custom_code_at_the_beginning: |
    std::vector<int64_t> size(2);
    auto symInt2Int = [](const c10::SymInt& t)-> int64_t {return t.expect_int();};
    std::vector<int64_t> grad_input_shape(input_size.size());
    std::transform(input_size.cbegin(), input_size.cend(), grad_input_shape.begin(), symInt2Int);
    auto grad_input = at::empty(grad_input_shape,grad_output.options(),$SUGGESTED_MEMORYFORMAT);
  custom_code_before_call_diopi: |
    if (output_size.size() > 0) {
      std::copy(output_sizeVector.begin(), output_sizeVector.end(), size.begin());
    } else {
      size[0] = std::floor((*(input_sizeVector.rbegin() + 1)) * scales_h.value_or(1.0));
      size[1] = std::floor((*(input_sizeVector.rbegin())) * scales_w.value_or(1.0));
    }
    const char* mode = "bilinear";
  interface: diopiUpsampleLinearBackward(ctx, grad_input, grad_output, size, input_size, align_corners, mode)

- schema: "sin(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiSin(ctx, out, self)

- schema: "sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSin(ctx, out, self)

- schema: "sin_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiSinInp(ctx, self)

- schema: "cos(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiCos(ctx, out, self)

- schema: "cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiCos(ctx, out, self)

- schema: "cos_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiCosInp(ctx, self)

- schema: "bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBmm(ctx, out, self, mat2)

- schema: "silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  custom_fallback: True
  interface: diopiSilu(ctx, out, self)

- schema: "reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiReciprocal(ctx, out, self)

- schema: "normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)"
  autocompare: disable
  interface: diopiNormalInp(ctx, self, mean, std, generator)

- schema: "mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMm(ctx, out, self, mat2)

- schema: "matmul(Tensor self, Tensor other) -> Tensor"
  device: [droplet]
  custom_code_at_the_beginning: |
    const auto shapeA = self.sizes();
    const auto shapeB = other.sizes();
    const int64_t nA= shapeA.size();
    const int64_t nB = shapeB.size();
    std::vector<int64_t> output_shape;
    if (nA == nB && nB == 2) {
        output_shape = {shapeA[0], shapeB[1]};
    } else if (nA == 1 && nB == 2) {
      output_shape = {shapeB[1]};
    } else if (nA == 2 && nB == 1) {
      output_shape = {shapeA[0]};
    } else if (nA > 2 && nB == 1) {
      output_shape = std::vector<int64_t>(shapeA.begin(), shapeA.end() - 1);
    } else if (nA == 1 && nB > 2) {
      output_shape = std::vector<int64_t>(shapeB.begin(), shapeB.end());
      output_shape.erase(output_shape.end() - 2);
    } else if (nA >= 2 && nB >= 2) {
      const int64_t nC = std::max(nA, nB);
      output_shape = std::vector<int64_t>(nC, 1);
      output_shape[output_shape.size() - 1] = shapeB[nB - 1];
      output_shape[output_shape.size() - 2] = shapeA[nA - 2];
      for (int i = 3; i <= nC; ++i) {
        int dim = nC - i;
        if (nA - i >= 0) {
          output_shape[dim] = std::max(output_shape[dim], shapeA[nA - i]);
        }
        if (nB - i >= 0) {
          output_shape[dim] = std::max(output_shape[dim], shapeB[nB - i]);
        }
      }
    }

    auto out = at::empty(output_shape, self.options());

  interface: diopiMatmul(ctx, out, self, other)

- schema: "matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  device: [droplet]
  interface: diopiMatmul(ctx, out, self, other)

- schema: "cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const auto self_dtype = at::native::to(self, dtype);
    ::diopiConstTensorHandle_t self_dtype_diopi = dipu::diopi_helper::toDiopiTensorHandle(self_dtype);
  interface: diopiCumsum(ctx, out, self_dtype_diopi, dim)

- schema: "flip(Tensor self, int[] dims) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
    ::diopiSize_t diopi_size = toDiopiSize(dims);
  interface: diopiFlip(ctx, out,self,diopi_size)

- schema: "linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLinspace(ctx,out,  start, end,  steps)

- schema: "gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiGather(ctx,out, self,  dim,  index)

- schema: "index_select(Tensor self, int dim, Tensor index) -> Tensor"
  custom_code_at_the_beginning: |
    auto shape = self.sizes();
    std::vector<int64_t> output_shape(shape.begin(), shape.end());
    dim += dim >= 0 ? 0 : shape.size();
    output_shape[dim] = index.numel();
    auto out = at::empty({output_shape}, self.options());
  interface: diopiIndexSelect(ctx, out, self, dim, index)

- schema: "hardswish(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty(self.sizes(), self.options());
  interface: diopiHardswish(ctx, out, self)

- schema: "hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiHardswish(ctx, out, self)

- schema: "hardswish_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiHardswishInp(ctx, self);

- schema: "hardswish_backward(Tensor grad_output, Tensor self) -> Tensor grad_input"
  custom_code_at_the_beginning: |
    auto grad_input = at::empty(self.sizes(), grad_output.options());
  interface: diopiHardswishBackward(ctx, grad_input, grad_output, self)

- schema: "sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSigmoid(ctx,out,self)

- schema: "sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiSigmoidBackward(ctx, grad_input, grad_output, output)

- schema: "binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBCELoss(ctx, out, self, target, weight, static_cast<diopiReduction_t>(reduction))

- schema: "binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor"
  custom_code_at_the_beginning: |
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor out;
    auto options = self.options();
    if (reductionDiopi == ReductionNone) {
      out = at::empty(self.sizes(), options);
    } else {
      out = at::empty({}, options);
    }
  interface: diopiBCELoss(ctx, out, self, target, weight, reductionDiopi)

- schema: "binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiBCELossBackward(ctx, grad_input, grad_output, self, target, weight, static_cast<diopiReduction_t>(reduction))

- schema: "ctc_loss_tensor_backward(Tensor grad_output, Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, int reduction=Mean, bool zero_infinity=False) -> Tensor grad_input"
  device: [camb]
  autocompare: disable
  register_op: False
  custom_code_at_the_beginning: |
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor grad_input = at::empty_like(log_probs);

  interface: diopiCTCLossBackward(ctx,  grad_input,  grad_output, log_probs, targets, input_lengths, target_lengths, neg_log_likelihood, log_alpha, blank, reductionDiopi, zero_infinity)

- schema: "ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor"
  device: [camb]
  autograd: True
  outs: [neg_log_likelihood, log_alpha]
  custom_code_at_the_beginning: |
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor out;
    int64_t batch_size = log_probs.size(1);
    int64_t num_labels = log_probs.size(2);
    int64_t max_target_length = target_lengths.max().cpu().item().to<int64_t>();
    auto options = log_probs.options();
    if (reductionDiopi == ReductionNone) {
      out = at::empty({batch_size}, options);
    } else {
      out = at::empty({1}, options);
    }
    at::Tensor neg_log_likelihood = at::empty({batch_size}, options);
    at::Tensor log_alpha = at::empty({batch_size, log_probs.size(0), 2 * max_target_length + 1}, options);
  interface: diopiCTCLoss(ctx, out, neg_log_likelihood, log_alpha, log_probs, targets, input_lengths, target_lengths, blank, reductionDiopi, zero_infinity)
  forward_process_code: |
    auto targets_dev = targets.to(log_probs.device());
    auto input_lengths_dev = input_lengths.to(log_probs.device());
    auto target_lengths_dev = target_lengths.to(log_probs.device());
  forward_schema: ctc_loss_tensor(Tensor log_probs, Tensor targets_dev, Tensor input_lengths_dev, Tensor target_lengths_dev, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
  backward_schema: "ctc_loss_tensor_backward(Tensor grad_output, Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, int reduction=Mean, bool zero_infinity=False) -> Tensor grad_input"
  saved_data:
    [
      log_probs,
      targets,
      input_lengths,
      target_lengths,
      blank,
      reduction,
      zero_infinity,
    ]
  cal_grad_code: |
    auto log_probs = log_probs_.toTensor();
    auto targets = targets_.toTensor().to(log_probs.device());
    auto input_lengths = input_lengths_.toTensor().to(log_probs.device());
    auto target_lengths = target_lengths_.toTensor().to(log_probs.device());
    auto blank = blank_.toInt();
    auto reduction = reduction_.toInt();
    auto zero_infinity = zero_infinity_.toBool();

    auto grad_output = grad_outputs.at(0);
    auto options = log_probs.options();

    int64_t batch_size = log_probs.size(1);
    int64_t num_labels = log_probs.size(2);
    int64_t max_target_length = target_lengths.max().cpu().item().to<int64_t>();

    at::Tensor neg_log_likelihood = at::empty({batch_size}, options);
    at::Tensor log_alpha = at::empty({batch_size, log_probs.size(0), 2 * max_target_length + 1}, options);
  backward_return_code: |
    std::vector<at::Tensor> outputs(7);
    outputs[0] = result;
    return outputs;

- schema: "ctc_loss_intlist_backward(Tensor grad_output, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, int reduction=Mean, bool zero_infinity=False) -> Tensor grad_input"
  device: [camb]
  autocompare: disable
  register_op: False
  ins: [input_lengths_tensor, target_lengths_tensor]
  custom_code_at_the_beginning: |
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);

    auto input_lengths_tensor = at::empty({static_cast<int64_t>(input_lengths.size())}, at::kLong);
    auto target_lengths_tensor =  at::empty({static_cast<int64_t>(target_lengths.size())}, at::kLong);
    std::copy(input_lengths.begin(), input_lengths.end(), static_cast<int64_t*>(input_lengths_tensor.data_ptr()));
    std::copy(target_lengths.begin(), target_lengths.end(), static_cast<int64_t*>(target_lengths_tensor.data_ptr()));

    input_lengths_tensor = input_lengths_tensor.to(log_probs.device());
    target_lengths_tensor = target_lengths_tensor.to(log_probs.device());

    at::Tensor grad_input = at::empty_like(log_probs);

  interface: diopiCTCLossBackward(ctx,  grad_input,  grad_output, log_probs, targets, input_lengths_tensor, target_lengths_tensor, neg_log_likelihood, log_alpha, blank, reductionDiopi, zero_infinity)

- schema: "ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor"
  device: [camb]
  autograd: True
  ins: [input_lengths_tensor, target_lengths_tensor]
  outs: [neg_log_likelihood, log_alpha]
  custom_code_at_the_beginning: |
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    auto input_lengths_tensor = at::empty({static_cast<int64_t>(input_lengths.size())}, at::kLong);
    auto target_lengths_tensor =  at::empty({static_cast<int64_t>(target_lengths.size())}, at::kLong);
    std::copy(input_lengths.begin(), input_lengths.end(), static_cast<int64_t*>(input_lengths_tensor.data_ptr()));
    std::copy(target_lengths.begin(), target_lengths.end(), static_cast<int64_t*>(target_lengths_tensor.data_ptr()));

    input_lengths_tensor = input_lengths_tensor.to(log_probs.device());
    target_lengths_tensor = target_lengths_tensor.to(log_probs.device());

    at::Tensor out;
    int64_t batch_size = log_probs.size(1);
    int64_t num_labels = log_probs.size(2);
    int64_t max_target_length = target_lengths_tensor.max().cpu().item().to<int64_t>();
    auto options = log_probs.options();
    if (reductionDiopi == ReductionNone) {
      out = at::empty({batch_size}, options);
    } else {
      out = at::empty({1}, options);
    }
    at::Tensor neg_log_likelihood = at::empty({batch_size}, options);
    at::Tensor log_alpha = at::empty({batch_size, log_probs.size(0), 2 * max_target_length + 1}, options);
  interface: diopiCTCLoss(ctx, out, neg_log_likelihood, log_alpha, log_probs, targets, input_lengths_tensor, target_lengths_tensor, blank, reductionDiopi, zero_infinity)
  backward_schema: "ctc_loss_intlist_backward(Tensor grad_output, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, int reduction=Mean, bool zero_infinity=False) -> Tensor grad_input"
  saved_data:
    [
      log_probs,
      targets,
      input_lengths,
      target_lengths,
      blank,
      reduction,
      zero_infinity,
    ]
  cal_grad_code: |
    auto log_probs = log_probs_.toTensor();
    auto targets = targets_.toTensor();
    auto input_lengths = input_lengths_.toIntVector();
    auto target_lengths = target_lengths_.toIntVector();
    auto blank = blank_.toInt();
    auto reduction = reduction_.toInt();
    auto zero_infinity = zero_infinity_.toBool();

    auto grad_output = grad_outputs.at(0);
    auto options = log_probs.options();

    int64_t batch_size = log_probs.size(1);
    int64_t num_labels = log_probs.size(2);
    // int64_t max_target_length = target_lengths_tensor.max().cpu().item().to<int64_t>();
    int64_t max_target_length = *std::max_element(target_lengths.begin(), target_lengths.end());

    at::Tensor neg_log_likelihood = at::empty({batch_size}, options);
    at::Tensor log_alpha = at::empty({batch_size, log_probs.size(0), 2 * max_target_length + 1}, options);
  backward_return_code: |
    std::vector<at::Tensor> outputs(7);
    outputs[0] = result;
    return outputs;

- schema: "_ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor neg_log_likelihood, Tensor log_alpha)"
  device: [-camb, all]
  custom_code_at_the_beginning: |
    int64_t batch_size = log_probs.size(1);
    int64_t num_labels = log_probs.size(2);
    int64_t max_target_length = target_lengths.max().cpu().item().to<int64_t>();
    auto neg_log_likelihood = at::empty({batch_size}, log_probs.options());
    auto log_alpha = at::empty({batch_size, log_probs.size(0), 2 * max_target_length + 1}, log_probs.options());
  interface: diopiCTCLoss(ctx, neg_log_likelihood, neg_log_likelihood, log_alpha, log_probs, targets, input_lengths, target_lengths, blank, ReductionNone, zero_infinity); # TODO: param log_alpha ?

- schema: "_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor neg_log_likelihood, Tensor log_alpha)"
  ins: [input_lengths_tensor, target_lengths_tensor]
  no_device_check_args: [input_lengths_tensor, target_lengths_tensor]
  device: [-camb, all]
  custom_code_at_the_beginning: |
    int64_t batch_size = log_probs.size(1);
    int64_t num_labels = log_probs.size(2);
    int64_t max_target_length = *std::max_element(target_lengths.begin(), target_lengths.end());;
    auto neg_log_likelihood = at::empty({batch_size}, log_probs.options());
    auto log_alpha = at::empty({batch_size, log_probs.size(0), 2 * max_target_length + 1}, log_probs.options());
    auto input_lengths_tensor = at::empty({static_cast<int64_t>(input_lengths.size())}, at::kLong);
    auto target_lengths_tensor =  at::empty({static_cast<int64_t>(target_lengths.size())}, at::kLong);
    std::copy(input_lengths.begin(), input_lengths.end(), static_cast<int64_t*>(input_lengths_tensor.data_ptr()));
    std::copy(target_lengths.begin(), target_lengths.end(), static_cast<int64_t*>(target_lengths_tensor.data_ptr()));
  interface: diopiCTCLoss(ctx, neg_log_likelihood, neg_log_likelihood, log_alpha, log_probs, targets, input_lengths_tensor, target_lengths_tensor, blank, ReductionNone, zero_infinity); # TODO: param log_alpha ?

- schema: "_ctc_loss_backward.Tensor(Tensor grad, Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor grad_input"
  device: [-camb, all]
  custom_code_at_the_beginning: |
    auto grad_input = at::empty(log_probs.sizes(), grad.options());
  interface: diopiCTCLossBackward(ctx, grad_input, grad, log_probs, targets, input_lengths, target_lengths, neg_log_likelihood, log_alpha, blank, ReductionNone, zero_infinity)

- schema: "_ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor grad_input"
  ins: [input_lengths_tensor, target_lengths_tensor]
  no_device_check_args: [input_lengths_tensor, target_lengths_tensor]
  device: [-camb, all]
  custom_code_at_the_beginning: |
    auto grad_input = at::empty(log_probs.sizes(), grad.options());
    auto input_lengths_tensor = at::empty({static_cast<int64_t>(input_lengths.size())}, at::kLong);
    auto target_lengths_tensor =  at::empty({static_cast<int64_t>(target_lengths.size())}, at::kLong);
    std::copy(input_lengths.begin(), input_lengths.end(), static_cast<int64_t*>(input_lengths_tensor.data_ptr()));
    std::copy(target_lengths.begin(), target_lengths.end(), static_cast<int64_t*>(target_lengths_tensor.data_ptr()));
  interface: diopiCTCLossBackward(ctx, grad_input, grad, log_probs, targets, input_lengths_tensor, target_lengths_tensor, neg_log_likelihood, log_alpha, blank, ReductionNone, zero_infinity)

- schema: "clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiClampMinScalar(ctx, out, self, min)

- schema: "clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiClampMin(ctx, out, self, min)

- schema: "clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)"
  interface: diopiClampMinInpScalar(ctx, self, min)

- schema: "clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)"
  interface: diopiClampMinInp(ctx, self, min)

- schema: "clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiClampMaxScalar(ctx, out, self, max)

- schema: "clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiClampMax(ctx, out, self, max)

- schema: "clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)"
  interface: diopiClampMaxInpScalar(ctx, self, max)

- schema: "clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)"
  interface: diopiClampMaxInp(ctx, self, max)

- schema: "minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  no_device_check_args: [self, other]
  ins: [selfTemp, otherTemp]
  custom_code_at_the_beginning: |
    auto selfTemp = (self.numel() == 1 && self.is_cpu()) ? self.to(other.device()) : self;
    auto otherTemp = (other.numel() == 1 && other.is_cpu()) ? other.to(self.device()) : other;
  interface: diopiMinimum(ctx, out, selfTemp, otherTemp)

- schema: "scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiScatterScalar(ctx, out, self, dim, value, index, "")

- schema: "scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiScatterScalar(ctx, out, self, dim, value, index, reduce.data())

- schema: "scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiScatter(ctx, out, self, dim, src, index, "")

- schema: "scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiScatter(ctx, out, self, dim, src, index, reduce.data())

- schema: "scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiScatter(ctx, out, self, dim, src, index, "add")

- schema: "remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiRemainderScalar(ctx, out, self, other)

- schema: "remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_remainder_scalar_out(self, other.item(), out);
    }
  interface: diopiRemainderTensor(ctx, out, self, other)

- schema: "norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    ::diopiSize_t dimDiopiSize = toDiopiSize(dim);
  interface: diopiNorm(ctx, out, self, p, dimDiopiSize);

- schema: linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
  custom_code_at_the_beginning: |
    ::diopiSize_t dimDiopiSize = toDiopiSize(dim);
  interface: diopiNorm(ctx, out, self, ord, dimDiopiSize);

- schema: "floor_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiFloorInp(ctx, self);

- schema: "floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiFloor(ctx, out, self);

- schema: "ceil_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiCeilInp(ctx, self);

- schema: "ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiCeil(ctx, out, self);

- schema: "arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (out.numel() == 0) {
      int64_t numel = std::floor((end.to<double>() - start.to<double>()) / step.to<double>());
      out.resize_({numel});
    }
  interface: diopiArange(ctx, out, start, end, step)

- schema: "index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -> Tensor(a!)"
  custom_fallback: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> indices_tensor_vec(indices.size());
    std::vector<diopiConstTensorHandle_t> indices_vec(indices.size(), nullptr);
    diopiTensorHandle_t out_ptr = nullptr;
    for (int i = 0; i < indices.size(); ++i) {
      indices_tensor_vec[i] = (indices[i].has_value() && indices[i].value().defined()) ? indices[i].value().to(self.device()) : at::Tensor();
      indices_vec[i] = diopi_helper::toDiopiTensorHandle(indices_tensor_vec[i]);
    }
  interface: diopiIndex(ctx, &out_ptr, self, indices_vec.data(), indices_vec.size())
  custom_code_before_return: |
    dipu::getCurrentDIPUStream().synchronize();
    out = *reinterpret_cast<at::Tensor*>(out_ptr);

- schema: "_index_put_impl_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -> Tensor(a!)"
  custom_fallback: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> indices_tensor_vec(indices.size());
    std::vector<diopiConstTensorHandle_t> indices_vec(indices.size(), nullptr);
    for (int i = 0; i < indices.size(); ++i) {
      indices_tensor_vec[i] = (indices[i].has_value() && indices[i].value().defined()) ? indices[i].value().to(self.device()) : at::Tensor();
      indices_vec[i] = diopi_helper::toDiopiTensorHandle(indices_tensor_vec[i]);
    }
  interface: diopiIndexPut(ctx, self, self, values, indices_vec.data(), indices_vec.size(), accumulate)

- schema: "_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor"
  custom_code_at_the_beginning: |
    int64_t* compute_mode_ptr = compute_mode.has_value() ? (&compute_mode.value()) : nullptr;
    auto x1_shape = x1.sizes().vec();
    auto x2_shape = x2.sizes().vec();
    std::fill(x1_shape.end() - 2, x1_shape.end(), 1);
    std::fill(x2_shape.end() - 2, x2_shape.end(), 1);
    auto output_shape = at::infer_size(x1_shape, x2_shape);
    *output_shape.rbegin() = x2.size(-2);
    *(output_shape.rbegin() + 1) = x1.size(-2);
    auto out = at::empty(output_shape, x1.options());
  interface: diopiCdist(ctx, out, x1, x2, p, compute_mode_ptr)

- schema: "_cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -> Tensor grad_input"
  custom_code_at_the_beginning: |
    auto x1_shape = x1.sizes().vec();
    auto x2_shape = x2.sizes().vec();
    std::fill(x1_shape.end() - 2, x1_shape.end(), 1);
    std::fill(x2_shape.end() - 2, x2_shape.end(), 1);
    auto grad_shape = at::infer_size(x1_shape, x2_shape);
    *grad_shape.rbegin() = x1.size(-1);
    *(grad_shape.rbegin() + 1) = x1.size(-2);
    auto grad_input = at::empty(grad_shape, grad.options());
  interface: diopiCdistBackward(ctx, grad_input, grad, x1, x2, p, cdist)

- schema: "erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiErfinv(ctx, out, self)

- schema: "erfinv_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiErfinvInp(ctx, self)

- schema: "polar(Tensor abs, Tensor angle) -> Tensor"
  custom_code_at_the_beginning: |
    auto dtype = c10::toComplexType(abs.scalar_type());
    auto out = at::empty(abs.sizes(), abs.options().dtype(dtype));
  interface: diopiPolar(ctx, out, abs, angle)

- schema: "polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiPolar(ctx, out, abs, angle)

- schema: "lerp.Scalar_out(Tensor input, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLerpScalar(ctx, out, input, end, weight)

- schema: "lerp.Tensor_out(Tensor input, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLerpTensor(ctx, out, input, end, weight)

- schema: "atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAtan(ctx, out, self)

- schema: "im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor"
  size_attr: [kernel_size, stride, padding, dilation]
  custom_code_at_the_beginning: |
    auto input_shape = self.sizes().vec();
    bool batched_input = true;
    if (input_shape.size() == 3) {
      batched_input = false;
      input_shape.insert(input_shape.begin(), 1);
    }

    int num_blocks = 1;
    for(int i = 0; i < 2; i++){
      num_blocks *= int((input_shape[i + 2] + 2 * padding[i] - dilation[i] * (kernel_size[i] - 1) - 1) / stride[i]) + 1;
    }
    int channels = input_shape[1];
    for(int i = 0; i < 2; i++){
      channels *= kernel_size[i];
    }

    std::vector<int64_t> out_shape({channels, num_blocks});
    if(batched_input == true){
      out_shape.insert(out_shape.begin(), input_shape[0]);
    }
    auto out = at::empty({out_shape}, self.options());
  interface: diopiIm2Col(ctx, out, self, kernel_size, dilation, padding, stride)

- schema: "col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor"
  size_attr: [kernel_size, stride, padding, dilation]
  custom_code_at_the_beginning: |
    auto input_shape = self.sizes().vec();
    bool batched_input = true;
    if (input_shape.size() == 2) {
      batched_input = false;
      input_shape.insert(input_shape.begin(), 1);
    }

    int channels = input_shape[1];
    for(int i = 0; i < 2; i++){
      channels = channels / kernel_size[i];
    }

    std::vector<int64_t> out_shape({channels, output_size.at(0).expect_int(), output_size.at(1).expect_int()});
    if(batched_input == true){
      out_shape.insert(out_shape.begin(), input_shape[0]);
    }
    auto out = at::empty({out_shape}, self.options());
  interface: diopiCol2Im(ctx, out, self, output_size, kernel_size, dilation, padding, stride)

- schema: "sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSgn(ctx, out,self)

- schema: "triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiTriu(ctx, out, self, diagonal)

- schema: "isnan(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty(self.sizes(), self.options().dtype(at::kBool));
  interface: diopiIsNan(ctx, out, self)

- schema: "embedding_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor grad_weight"
  custom_code_at_the_beginning: |
    auto grad_weight = at::zeros({num_weights, grad.size(-1)}, grad.options());
  interface: diopiEmbeddingBackward(ctx, grad_weight, grad, indices, num_weights, padding_idx, scale_grad_by_freq, sparse)

- schema: "sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSign(ctx, out, self)

- schema: linalg_qr.out(Tensor A, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
  interface: diopiLinalgQR(ctx, A, mode.data(), Q, R)

- schema: amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
  custom_code_at_the_beginning: |
    ::diopiSize_t dimDiopiSize = toDiopiSize(dim);
  interface: diopiAmax(ctx, out, self, dimDiopiSize, keepdim)

- schema: batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)
  custom_code_at_the_beginning: |
    auto shape = input.size(1);
    auto out0 = at::empty({shape}, input.options().dtype(at::kFloat), $SUGGESTED_MEMORYFORMAT);
    auto out1 = at::empty({shape}, input.options().dtype(at::kFloat), $SUGGESTED_MEMORYFORMAT);
  interface: diopiBatchNormStats(ctx, out0, out1, input, eps)

- schema: batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
  custom_code_at_the_beginning: |
    auto shape = input.size(1);
    auto out0 = at::empty({shape}, input.options().dtype(at::kFloat));
    auto out1 = at::empty({shape}, input.options().dtype(at::kFloat));
  interface: diopiBatchNormGatherStatsWithCounts(ctx, out0, out1, input, mean, invstd, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), momentum, eps, counts)

- schema: batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)
  custom_code_at_the_beginning: |
    auto shape = input.size(1);
    at::Tensor out0;
    at::Tensor out1;
    at::Tensor out2;
    at::Tensor out3;
    if(input_g){
      out0 = at::empty({shape}, input.options().dtype(at::kFloat), $SUGGESTED_MEMORYFORMAT);
      out1 = at::empty({shape}, input.options().dtype(at::kFloat), $SUGGESTED_MEMORYFORMAT);
    }
    if(weight_g){
      out2 = at::empty({shape}, input.options().dtype(at::kFloat));
    }
    if(bias_g){
      out3 = at::empty({shape}, input.options().dtype(at::kFloat));
    }
  interface: diopiBatchNormBackwardReduce(ctx, out0, out1, out2, out3, grad_out, input, mean, invstd, weight, input_g, weight_g, bias_g)

- schema: batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor sum_dy, Tensor sum_dy_xmu, Tensor count) -> Tensor
  custom_code_at_the_beginning: |
    auto out = at::empty_like(grad_out, grad_out.options(), $SUGGESTED_MEMORYFORMAT);
  interface: diopiBatchNormBackwardElemt(ctx, out, grad_out, input, mean, invstd, weight, sum_dy, sum_dy_xmu, count);

- schema: batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor
  custom_code_at_the_beginning: |
    auto out = at::empty_like(input, input.options(), $SUGGESTED_MEMORYFORMAT);
  interface: diopiBatchNormElemt(ctx, out, input, weight, bias, mean, invstd, eps);

- schema: smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiSmoothL1Loss(ctx, out, self, target, static_cast<diopiReduction_t>(reduction), static_cast<double>(beta));

- schema: smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
  interface: diopiSmoothL1LossBackward(ctx, grad_input, grad_output, self, target, static_cast<diopiReduction_t>(reduction), static_cast<double>(beta));

- schema: _foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1)->()
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_add__tensor(selfVec[i], other.at(i), alpha);
    }
    return;
  interface: diopiAddInp(ctx, self, other, alpha)

- schema: _foreach_add.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -> Tensor[]
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> out(self.size());
    for (size_t i = 0;i < self.size();i++) {
      auto& in = self[i];
      out[i] = at::empty(in.sizes(), in.options());
      dipu_add_out(self.at(i), other.at(i), alpha, out[i]);
    }
    return out;
  interface: diopiAdd(ctx, self, other, alpha)

- schema: _foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_add__scalar(selfVec[i], scalar, 1.0);
    }
    return;
  interface: diopiAddInpScalar(ctx, self, other, alpha)

- schema: _foreach_add.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> out(self.size());
    for (size_t i = 0;i < self.size();i++) {
      out[i] = at::empty(self[i].sizes(), self[i].options());
      dipu_add_scalar_out(self[i], scalar, 1.0 , out[i]);
    }
    return out;
  interface: diopiAddScalar(ctx, out, self, other, alpha)

- schema: _foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_mul__tensor(selfVec[i], other.at(i));
    }
    return;
  interface: diopiMulInp(ctx, self, other, alpha)

- schema: _foreach_mul.List(Tensor[] self, Tensor[] other) -> Tensor[]
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> out(self.size());
    for (size_t i = 0;i < self.size();i++) {
      out[i] = at::empty_like(self[i]);
      dipu_mul_out(self[i], other[i], out[i]);
    }
    return out;
  interface: diopiMul(ctx, out, self, other)

- schema: _foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_mul__scalar(selfVec[i], scalar);
    }
    return;
  interface: diopiMulInpScalar(ctx, self, other, alpha)

- schema: _foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_mul__scalar(selfVec[i], scalars[i]);
    }
    return;
  interface: diopiMulInpScalar(ctx, self, other, alpha)

- schema: _foreach_mul.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> out(self.size());
    for (size_t i = 0;i < self.size();i++) {
      auto& in = self[i];
      out[i] = at::empty(in.sizes(), in.options());
      dipu_mul_scalar_out(self.at(i), scalar, out[i]);
    }
    return out;
  interface: diopiMulScalar(ctx, out, self, other)

- schema: _foreach_mul.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> out(self.size());
    for (size_t i = 0;i < self.size();i++) {
      auto& in = self[i];
      out[i] = at::empty(in.sizes(), in.options());
      dipu_mul_scalar_out(self.at(i), scalars[i], out[i]);
    }
    return out;
  interface: diopiMulScalar(ctx, out, self, other)

- schema: _foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_div__tensor(selfVec[i], other.at(i));
    }
    return;
  interface: diopiDivInp(ctx, self, other, alpha)

- schema: _foreach_div.List(Tensor[] self, Tensor[] other) -> Tensor[]
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> out(self.size());
    for (size_t i = 0;i < self.size();i++) {
      out[i] = at::empty_like(self[i]);
      dipu_div_out(self[i], other[i], out[i]);
    }
    return out;
  interface: diopiDiv(ctx, out, self, other)

- schema: _foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_div__scalar(selfVec[i], scalar);
    }
    return;
  interface: diopiDivInpScalar(ctx, self, other, alpha)

- schema: _foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_div__scalar(selfVec[i], scalars[i]);
    }
    return;
  interface: diopiDivInpScalar(ctx, self, other, alpha)

- schema: _foreach_div.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> out(self.size());
    for (size_t i = 0;i < self.size();i++) {
      auto& in = self[i];
      out[i] = at::empty(in.sizes(), in.options());
      dipu_div_scalar_out(self.at(i), scalar, out[i]);
    }
    return out;
  interface: diopiDivScalar(ctx, out, self, other)

- schema: _foreach_div.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> out(self.size());
    for (size_t i = 0;i < self.size();i++) {
      auto& in = self[i];
      out[i] = at::empty(in.sizes(), in.options());
      dipu_div_scalar_out(self.at(i), scalars[i], out[i]);
    }
    return out;
  interface: diopiDivScalar(ctx, out, self, other)

- schema: "_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()"
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_addcmul_(selfVec[i], tensor1[i], tensor2[i], scalars[i]);
    }
    return;
  interface: diopiAddcmulInp(ctx, self, tensor1, tensor2, scalars)

- schema: "_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()"
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_addcmul_(selfVec[i], tensor1[i], tensor2[i], value);
    }
    return;
  interface: diopiAddcmulInp(ctx, self, tensor1, tensor2, scalars)

- schema: "_foreach_addcmul_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()"
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    auto scalarsCpu = scalars.cpu();
    for (size_t i = 0;i < self.size();i++) {
      dipu_addcmul_(selfVec[i], tensor1[i], tensor2[i], scalarsCpu[i].item());
    }
    return;
  interface: diopiAddcmulInp(ctx, self, tensor1, tensor2, scalars)

- schema: "_foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()"
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_addcdiv_(selfVec[i], tensor1[i], tensor2[i], scalars[i]);
    }
    return;
  interface: diopiAddcdivInp(ctx, self, tensor1, tensor2, scalars)

- schema: "_foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()"
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_addcdiv_(selfVec[i], tensor1[i], tensor2[i], value);
    }
    return;
  interface: diopiAddcdivInp(ctx, self, tensor1, tensor2, scalars)

- schema: "_foreach_addcdiv_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()"
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    auto scalarsCpu = scalars.cpu();
    for (size_t i = 0;i < self.size();i++) {
      dipu_addcdiv_(selfVec[i], tensor1[i], tensor2[i], scalarsCpu[i].item());
    }
    return;
  interface: diopiAddcdivInp(ctx, self, tensor1, tensor2, scalars)

- schema: "_foreach_sqrt_(Tensor(a!)[] self) -> ()"
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    auto selfVec = self.vec();
    for (size_t i = 0;i < self.size();i++) {
      dipu_sqrt_(selfVec[i]);
    }
    return;
  interface: diopiSqrtInp(ctx, self)

- schema: "_foreach_sqrt(Tensor[] self) -> Tensor[]"
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> out(self.size());
    for (size_t i = 0;i < self.size();i++) {
      out[i] = at::empty_like(self[i]);
      dipu_sqrt_out(self[i], out[i]);
    }
    return out;
  interface: diopiSqrt(ctx, out, self)

- schema: _foreach_neg(Tensor[] self) -> Tensor[]
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> out(self.size());
    for (size_t i = 0;i < self.size();i++) {
      auto& in = self[i];
      out[i] = at::empty(in.sizes(), in.options());
      dipu_neg_out(self.at(i), out[i]);
    }
    return out;
  interface: diopiNeg(ctx, out, self)

- schema: _foreach_norm.Scalar(Tensor[] self, Scalar ord=2) -> Tensor[]
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    std::vector<at::Tensor> out(self.size());
    for (size_t i = 0;i < self.size();i++) {
      auto& in = self[i];
      out[i] = at::empty({}, in.options());
      dipu_norm_out(in, ord, {}, false, out[i]);
    }
    return out;
  interface: diopiNorm(ctx, out, self, p, dimDiopiSize);

# wrap_diopi_cast_dtype has no corresponding aten op and not registed, it's just a diopi func wrapper.
# use this tricky method to support call multiple diopi-op in one aten-op
- schema: "wrap_diopi_cast_dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)"
  register_op: False
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self, self.options().dtype(dtype));
  interface: diopiCastDtype(ctx, out, self);

# a diopi func wrapper.
- schema: wrap_diopi_copy_inp(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
  register_op: False
  no_device_check_args: [self, src]
  interface: diopiCopyInp(ctx, src, self)

# this copy_ aten op may use both diopiCastDtype and diopiCopyInp. it's a proxy/composite op
- schema: copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
  dummy_call_diopi: True
  custom_fallback: True
  device: [cuda, camb, ascend, droplet, supa]
  custom_code_at_the_beginning: |
    dipu::getDipuCopyInstance()->run(self, src, non_blocking);
    return self;
    // need add [composite] attr? the code behind this is useless.
  interface: diopiCopyInp(ctx, srcTemp, self)

# vendor who has no fully implemented diopi and proper fallback DIPUCopy sub-class
- schema: copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
  custom_fallback: True
  dummy_call_diopi: True
  custom_code_at_the_beginning: |
    return custom_fallback_dipu_copy_(self, src, non_blocking);
  device: [topsrider]
  interface: diopiCopyInp(ctx, src, self)

- schema: _amp_foreach_non_finite_check_and_unscale_(at::TensorList self, Tensor(b!) found_inf, Tensor inv_scale) -> void
  custom_fallback: True
  custom_code_at_the_beginning: |
    std::vector<diopiTensorHandle_t> diopiTensorHandles(self.size(), nullptr);
    std::transform(self.begin(), self.end(), diopiTensorHandles.begin(), [](const at::Tensor& t){
        return dipu::diopi_helper::toDiopiTensorHandle(const_cast<at::Tensor&>(t));
    });
  interface: diopiAmpForeachNonFiniteCheckAndUnscaleInp(ctx, diopiTensorHandles.data(), self.size(), found_inf, inv_scale)
  autocompare: disable

- schema: _amp_update_scale_(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval) -> Tensor(a!)
  custom_fallback: True
  interface: diopiAmpUpdateScaleInp(ctx, self, growth_tracker, found_inf, scale_growth_factor, scale_backoff_factor, growth_interval)
