- schema: "exampleop.overloadname(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  autocompare: disable
  register_op: False  # Whether generate registe code for this op, default value is True
  print_func_call_info: False # whether generate code that prints function call information
  dummy_call_diopi: False # Does not generate code that actually calls the diopi function, defalut value is False
  custom_code_at_the_beginning: "/* Here can be a piece of c++ code at the begining*/"
  custom_code_before_call_diopi: >
    std::cout << "self:" << self << std::endl;
    std::cout << "other:" << other << std::endl;
  custom_code_before_return: >
    dipu::getCurrentDIPUStream().synchronize();
    std::cout << "out:" << out << std::endl;
  interface: diopiAddScalar(ctx, out, self, other, alpha)

- schema: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
  interface: diopiFill(ctx, self, value)

- schema: "aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAddScalar(ctx, out, self, other, alpha)

- schema: "add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)"
  register_op: False
  custom_code_at_the_beginning: >
    TORCH_CHECK(at::can_cast(self.scalar_type(), other.type()));
  interface: diopiAddInpScalar(ctx, self, other, alpha)

- schema: "add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)"
  register_op: False
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_add__scalar(self, other.item(), alpha);
    }
  interface: diopiAddInp(ctx, self, other, alpha)

- schema: "aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_add_scalar_out(self, other.item(), alpha, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        if (alpha.toDouble() == 1.0) {
          return dipu_add_scalar_out(other, self.item(), alpha, out);
        } else {
          at::Tensor selfTensor = at::empty_like(other);
          dipu_fill__scalar(selfTensor, self.item());
          return dipu_add_out(selfTensor, other, alpha, out);
        }
    }
  interface: diopiAdd(ctx, out, self, other, alpha)

- schema: "aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSubScalar(ctx, out, self, other, alpha)

- schema: "aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_sub_scalar_out(self, other.item(), alpha, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        at::Tensor selfTensor = at::empty_like(other);
        dipu_fill__scalar(selfTensor, self.item());
        return dipu_sub_out(selfTensor, other, alpha, out);
    }
  interface: diopiSub(ctx, out, self, other, alpha)

- schema: "div.Scalar(Tensor self, Scalar other) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiDivScalar(ctx, out, self, other, RoundModeNone)

- schema: "div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiDivInpScalar(ctx, self, other, RoundModeNone)

- schema: "aten::div.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiDivScalar(ctx, out, self, other, RoundModeNone)

- schema: "div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div__scalar(self, other.item());
    }
  interface: diopiDivInp(ctx, self, other, RoundModeNone)

- schema: "aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_div_scalar_out(other, self.item(), out);
    }
  interface: diopiDiv(ctx, out, self, other, RoundModeNone)

- schema: "aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDivScalar(ctx, out, self, other, mode)

- schema: "aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div_scalar_mode_out(self, other.item(), rounding_mode, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_div_scalar_mode_out(other, self.item(), rounding_mode, out);
    }
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDiv(ctx, out, self, other, mode)

- schema: "mul.Scalar(Tensor self, Scalar other) -> Tensor"
  #register_op: False  # camb impl has bug
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  interface: diopiMulScalar(ctx, out, self, other)

- schema: "mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  #register_op: False
  interface: diopiMulInpScalar(ctx, self, other)

- schema: "aten::mul.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  #register_op: False  # camb impl has bug
  interface: diopiMulScalar(ctx, out, self, other)

- schema: "mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  #register_op: False  # camb impl has bug
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_mul__scalar(self, other.item());
    }
  interface: diopiMulInp(ctx, self, other)

- schema: "mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  #register_op: False  # camb impl has bug
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_mul_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_mul_scalar_out(other, self.item(), out);
    }
  interface: diopiMul(ctx, out, self, other)

- schema: "aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  register_op: False
  #custom_fallback: True
  #force_fallback: False
  interface: diopiBatchNorm(ctx, out, save_mean, save_invstd, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);

- schema: "aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)"
  register_op: False
  #custom_fallback: True
  #force_fallback: False
  custom_code_at_the_beginning: |
    const int64_t dim_c = input.size(1);
    auto out0 = at::empty_like(input);
    auto options = input.options().dtype(at::kFloat);
    auto out1 = at::empty({dim_c}, options);
    auto out2 = at::empty({dim_c}, options);
  interface: diopiBatchNorm(ctx, out0, out1, out2, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);

- schema: "native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  register_op: False
  #custom_fallback: True
  #force_fallback: True
  custom_code_at_the_beginning: |
    int64_t dim_c = input.size(1);
    auto options = input.options().dtype(at::kFloat);
    at::Tensor out0 = at::empty(input.sizes(), input.options());
    at::Tensor out1 = at::empty({dim_c}, options);
    at::Tensor out2 = at::empty({dim_c}, options);
  interface: diopiBatchNormBackward(ctx, out0, out1, out2, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps)

- schema: "native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor out, Tensor save_mean, Tensor save_invstd)"
  custom_code_at_the_beginning: |
    const auto input_shape = input.sizes();
    std::vector<int64_t> stat_shape(input_shape.cbegin(), input_shape.cbegin() + (input_shape.size() - normalized_shape.size()));
    auto options = input.options().dtype(at::kFloat);
    auto save_mean = at::empty(stat_shape, options);
    auto save_invstd = at::empty(stat_shape, options);
    auto out = at::empty_like(input);
  interface: diopiLayerNorm(ctx,  out,  save_mean,  save_invstd,  input,  weight,  bias, normalized_shape, eps);

- schema: "native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  custom_code_at_the_beginning: |
    auto options = grad_out.options();
    auto grad_input = output_mask[0] ? at::empty(input.sizes(), options) : at::Tensor();
    auto grad_weight = output_mask[1] ? at::empty(weight.value().sizes(), options) : at::Tensor();
    auto grad_bias = output_mask[2] ? at::empty(bias.value().sizes(), options) : at::Tensor();
  interface: diopiLayerNormBackward(ctx, grad_input, grad_weight, grad_bias, grad_out, input, weight,  bias, mean, rstd, normalized_shape);

- schema: "aten::native_layer_norm_backward.out(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  interface: diopiLayerNormBackward(ctx, grad_input, grad_weight, grad_bias, grad_out, input, weight,  bias, mean, rstd, normalized_shape);

- schema: "adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAdaptiveAvgPool2d(ctx, out, self, output_size)

- schema: "_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor"
  custom_code_at_the_beginning: |
    TORCH_CHECK(output_size.size() == 2, __func__, ":", __FILE__, ":", __LINE__,
        " output_size should equal 2, size is ", output_size.size());
    auto out_tensor_size = self.sizes().vec();
    out_tensor_size[self.dim() - 2] = output_size[0].expect_int();
    out_tensor_size[self.dim() - 1] = output_size[1].expect_int();
    at::Tensor out = at::empty(out_tensor_size, self.options());
  interface: diopiAdaptiveAvgPool2d(ctx, out, self, output_size)

- schema: "_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiAdaptiveAvgPool2dBackward(ctx, out, grad_output, self);

- schema: "eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiEqScalar(ctx, out, self, other)

- schema: "eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_eq_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_eq_scalar_out(other, self.item(), out);
    }
  interface: diopiEq(ctx, out, self, other)

- schema: "eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiEqInpScalar(ctx, self, other)

- schema: "eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_eq__scalar(self, other.item());
    }
  interface: diopiEqInp(ctx, self, other)

- schema: "lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLtScalar(ctx, out, self, other)

- schema: "lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_lt_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_lt_scalar_out(other, self.item(), out);
    }
  interface: diopiLt(ctx, out, self, other)

- schema: "lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiLtInpScalar(ctx, self, other)

- schema: "lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_lt__scalar(self, other.item());
    }
  interface: diopiLtInp(ctx, self, other)

- schema: "ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiNeScalar(ctx, out, self, other)

- schema: "ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ne_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_ne_scalar_out(other, self.item(), out);
    }
  interface: diopiNe(ctx, out, self, other)

- schema: "ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiNeInpScalar(ctx, self, other)

- schema: "ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ne__scalar(self, other.item());
    }
  interface: diopiNeInp(ctx, self, other)

- schema: "ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiGeScalar(ctx, out, self, other)

- schema: "ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ge_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_ge_scalar_out(other, self.item(), out);
    }
  interface: diopiGe(ctx, out, self, other)

- schema: "ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiGeInpScalar(ctx, self, other)

- schema: "ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ge__scalar(self, other.item());
    }
  interface: diopiGeInp(ctx, self, other)

- schema: "gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiGtScalar(ctx, out, self, other)

- schema: "gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_gt_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_gt_scalar_out(other, self.item(), out);
    }
  interface: diopiGt(ctx, out, self, other)

- schema: "gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiGtInpScalar(ctx, self, other)

- schema: "gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_gt__scalar(self, other.item());
    }
  interface: diopiGtInp(ctx, self, other)

- schema: "le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLeScalar(ctx, out, self, other)

- schema: "le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_le_scalar_out(self, other.item(), out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_le_scalar_out(other, self.item(), out);
    }
  interface: diopiLe(ctx, out, self, other)

- schema: "le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiLeInpScalar(ctx, self, other)

- schema: "le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_le__scalar(self, other.item());
    }
  interface: diopiLeInp(ctx, self, other)

- schema: "relu_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiReluInp(ctx, self)

- schema: "relu(Tensor self) -> Tensor"
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  interface: diopiRelu(ctx, out, self)

- schema: "randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiRandperm(ctx, out, n, 0)

- schema: "randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiRandperm(ctx, out, n, seed)

- schema: "aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    ::diopiSize_t diopi_size = toDiopiSize(dim);
  interface: diopiSum(ctx, out, self, diopi_size)

- schema: "addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
  interface: diopiAddmm(&context, out, self, mat1, mat2, beta, alpha)

- schema: "cross_entropy_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor"
  register_op: False
  custom_code_at_the_beginning: |
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor out = at::empty_like(self);
  interface: diopiCrossEntropyLossBackward(ctx, out, grad_output, self, target, weight, reductionDiopi, ignore_index.expect_int(), label_smoothing)


- schema: "cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor"
  autograd: True
  custom_code_at_the_beginning: |
    const int64_t ignore_index_int = ignore_index.expect_int();
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor out;
    auto options = self.options();
    if (reductionDiopi == ReductionNone) {
      out = at::empty(target.sizes(), options);
    } else {
      out = at::empty({}, options);
    }
  interface: diopiCrossEntropyLoss(ctx, out, self, target, weight, reductionDiopi, ignore_index_int, label_smoothing)
  backward_schema: "cross_entropy_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor"
  saved_data: [reduction, ignore_index, label_smoothing, weight, self, target]
  cal_grad_code: >
    auto grad_output = grad_outputs.at(0);
    auto reduction = reduction_.toInt();
    auto ignore_index = ignore_index_.toInt();
    auto label_smoothing = label_smoothing_.toDouble();
    auto weight = weight_.toOptional<at::Tensor>();
    auto self = self_.toTensor();
    auto target = target_.toTensor();
  backward_return_code: >
    std::vector<at::Tensor> outputs(6);
    outputs[0] = result;
    return outputs;


- schema: "convolution_backward(Tensor grad_output, Tensor input, Tensor weight, int[] bias_sizes, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  register_op: False
  size_attr: [stride, padding, dilation, bias_sizes]
  custom_code_at_the_beginning: |
    at::Tensor grad_input;
    at::Tensor grad_weight;
    at::Tensor grad_bias;
    grad_input = at::empty(input.sizes(), input.options());
    grad_weight = at::empty(weight.sizes(), weight.options().dtype(at::kFloat));
    if (output_mask[2]) {
        c10::IntArrayRef bias_size = { grad_output.size(1) };
        grad_bias = at::empty(bias_size, grad_output.options());
    }
    ::diopiSize_t output_paddingDiopiSize;
  custom_code_before_call_diopi: >
    ::diopiSize_t* bias_sizes_ptr = output_mask[2] ? &bias_sizesDiopiSize : nullptr;
  interface: diopiConvolution2dBackward(ctx, grad_input, grad_weight, grad_bias, grad_output, input, weight, bias_sizes_ptr, stride, padding, dilation, false, output_paddingDiopiSize, groups);

- schema: conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor
  size_attr: [stride, padding, dilation]
  custom_code_at_the_beginning: |
    int64_t batch_size = input.size(0);
    int64_t height = input.size(2);
    int64_t width = input.size(3);
    int64_t out_channel = weight.size(0);
    auto kernel_size = weight.sizes().slice(2);
    int64_t out_height = (height + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1;
    int64_t out_width = (width + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1;
    c10::SmallVector<int64_t, 8> output_size = {batch_size, out_channel, out_height, out_width};
    at::Tensor out = at::empty(output_size, input.options());
  interface: diopiConvolution2d(&context, out, input, weight, bias, stride, padding, dilation, groups)
  autograd: True
  forward_process_code: >
    bool bias_has_value = (bias.has_value() == true) ? bias.value().requires_grad() : false;
  saved_data: [stride, padding, dilation, groups, bias_has_value, input, weight]
  backward_schema: "convolution_backward(Tensor grad_output, Tensor input, Tensor weight, int[] bias_sizes, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  cal_grad_code: >
      auto grad_output = grad_outputs.at(0);
      auto input = input_.toTensor();
      auto weight = weight_.toTensor();
      auto padding = padding_.toIntVector();
      auto stride = stride_.toIntVector();
      auto dilation = dilation_.toIntVector();
      bool bias_has_value = bias_has_value_.toBool();
      auto groups = groups_.toInt();
      std::vector<int64_t> bias_sizes;
      if (bias_has_value) {
        bias_sizes.push_back(grad_output.size(1));
      }
      std::array<bool, 3> output_mask;
      output_mask[0] = input.requires_grad();
      output_mask[1] = weight.requires_grad();
      output_mask[2] = bias_has_value;
      bool transposed = false;
      at::IntArrayRef output_padding{};
  backward_return_code: >
    std::vector<at::Tensor>  outputs = {
          std::get<0>(result), std::get<1>(result), std::get<2>(result),
          at::Tensor(), at::Tensor(), at::Tensor(), at::Tensor()};
    return outputs;

- schema: "dropout_impl(Tensor input, float p, bool train, *, Tensor(a!) mask) -> Tensor"
  custom_code_at_the_beginning: |
    at::Tensor out = at::empty_like(input);
  register_op: False
  interface: diopiDropout(ctx, out, mask, input, p, train)

- schema: "dropout(Tensor input, float p, bool train) -> Tensor"
  custom_code_at_the_beginning: |
    auto mask = at::empty(input.sizes(), input.options().dtype(at::kByte));
    at::Tensor out = at::empty_like(input);
  interface: diopiDropout(ctx, out, mask, input, p, train)
  outs: [mask]
  autograd: True
  saved_data: [p, mask]
  forward_schema: "dropout_impl(Tensor input, float p, bool train, *, Tensor(a!) mask) -> Tensor"
  forward_process_code: >
    auto mask = at::empty(input.sizes(), input.options().dtype(at::kByte));
    at::Tensor out = at::empty_like(input);
  cal_grad_code: >
        auto p = p_.toDouble();
        double p1m = 1. - p;
        double scale = p1m == 0 ? 0. : 1. / p1m;
        auto mask = mask_.toTensor();
        at::Tensor out = grad_outputs[0] * mask * scale;
  backward_return_code: >
    std::vector<at::Tensor> outputs(6);
    outputs[0] = out;
    return outputs;

- schema: "dropout__impl(Tensor(a!) self, Tensor(b!) mask, float p, bool train) -> Tensor(a!)"
  register_op: False
  interface: diopiDropoutInp(ctx, self, mask, p, train)

- schema: "dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    auto mask = at::empty(self.sizes(), self.options().dtype(at::kByte));
  outs: [mask]
  interface: diopiDropoutInp(ctx, self, mask, p, train)
  autograd: True
  forward_process_code: >
    auto mask = at::empty(self.sizes(), self.options().dtype(at::kByte));
  saved_data: [p, mask]
  forward_schema: "dropout__impl(Tensor(a!) self, Tensor(b!) mask, float p, bool train) -> Tensor(a!)"
  cal_grad_code: >
    auto p = p_.toDouble();
    double p1m = 1. - p;
    double scale = p1m == 0 ? 0. : 1. / p1m;
    auto mask = mask_.toTensor();
    at::Tensor out = grad_outputs[0] * mask * scale;
  backward_return_code: >
    std::vector<at::Tensor> outputs(6);
    outputs[0] = out;
    return outputs;
  wrappter_custom_return: return self;



- schema: "log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLog(ctx, out, self)

- schema: "log_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiLogInp(ctx, self)

- schema: "log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLog2(ctx, out, self)

- schema: "log2_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiLog2Inp(ctx, self)

- schema: "abs(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiAbs(ctx, out, self)

- schema: "abs_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiAbsInp(ctx, self)

- schema: "abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAbs(ctx, out, self)

- schema: "neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiNeg(ctx, out, self)

- schema: "neg_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiNegInp(ctx, self)

- schema: "sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSqrt(ctx, out, self)

- schema: "sqrt_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiSqrtInp(ctx, self)

- schema: "all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAll(ctx, out, self, &dim)

- schema: "all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAll(ctx, out, self, nullptr)

- schema: "any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAny(ctx, out, self, nullptr)

- schema: "any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAny(ctx, out, self, &dim)

- schema: "topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  interface: diopiTopk(ctx, values, indices, self, k, dim, largest, sorted)

- schema: "topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)"
  custom_code_at_the_beginning: |
    std::vector<int64_t> output_size(self.sizes().begin(), self.sizes().end());
    dim = dim < 0 ? (dim + output_size.size()) : dim;
    output_size[dim] = k;
    auto values = at::empty(output_size, self.options());
    auto indices = at::empty(output_size, self.options().dtype(at::kLong));
  interface: diopiTopk(ctx, values, indices, self, k, dim, largest, sorted)

- schema: "mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    ::diopiSize_t diopi_size = toDiopiSize(dim);
  interface: diopiMean(ctx, out, self, diopi_size);

- schema: "linear_backward(Tensor input, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  register_op: False
  custom_code_at_the_beginning: |
    auto grad_input = at::empty(input.sizes(), input.options());
    auto grad_weight = at::empty(weight.sizes(), weight.options().dtype(at::kFloat));
    at::Tensor grad_bias;
    bool bias_has_value = output_mask[2];
    if (bias_has_value) {
      grad_bias = at::empty({grad_output.size(-1)}, grad_output.options());
    }
  interface: diopiLinearBackward(ctx, grad_input, grad_weight, grad_bias, grad_output, input, weight)

- schema: "linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
  autograd: True
  custom_code_at_the_beginning: |
    std::vector<int64_t> output_size(input.sizes().begin(), input.sizes().end());
    output_size.back() = weight.sizes()[0];
    auto out = at::empty(output_size, input.options());
  interface: diopiLinear(ctx, out, input, weight, bias)
  forward_process_code: >
    bool bias_has_value = (bias.has_value() == true) ? bias.value().requires_grad() : false;
    std::array<bool, 3> output_mask{input.requires_grad(), weight.requires_grad(), bias_has_value};
  saved_data: [output_mask, input, weight]
  cal_grad_code: >
    auto output_mask = output_mask_.to<std::array<bool, 3>>();
    auto input = input_.toTensor();
    auto weight = weight_.toTensor();
    auto grad_output = grad_outputs[0];
  backward_schema: "linear_backward(Tensor input, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  backward_return_code: >
    return {std::get<0>(result), std::get<1>(result), std::get<2>(result)};
- schema: "_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLogSoftmaxBackward(ctx, out, grad_output, output, dim)

- schema: "_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLogSoftmax(ctx, out, self, dim)

- schema: "aten::log_softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLogSoftmax(ctx, out, self, dim)

- schema: "max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))"
  size_attr: [kernel_size, stride, padding, dilation]
  interface: diopiMaxPool2dWithIndices(&context, out, indices, self, kernel_size, stride, padding, dilation, ceil_mode)

- schema: max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
  size_attr: [kernel_size, stride, padding, dilation]
  interface: diopiMaxPool2dBackward(ctx, grad_input, grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices)

- schema: nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiNLLLoss(ctx, out, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int())

- schema: nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
  interface: diopiNLLLoss(ctx, output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int())

- schema: nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
  interface: diopiNLLLossBackward(&context, grad_input, grad_output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int());

- schema: "threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiThresholdBackward(ctx, grad_input, grad_output, self, &threshold)

- schema: "transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)"
  register_op: False
  custom_code_at_the_beginning: |
    std::vector<int64_t> output_size(self.sizes().cbegin(), self.sizes().cend());
    std::vector<int64_t> output_stride(self.strides().cbegin(), self.strides().cend());
    std::swap(output_size[dim0], output_size[dim1]);
    std::swap(output_stride[dim0], output_stride[dim1]);
    self.sizes() = output_size;
    self.strides() = output_stride;
    return self;
  interface: diopiTranspose(ctx, self, self, dim0, dim1)

- schema: "bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseAnd(ctx, out, self, other)

- schema: "bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  interface: diopiBitwiseAndInp(ctx, self, other)

- schema: "bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseAndScalar(ctx, out, self, other)

- schema: "bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiBitwiseAndInpScalar(ctx, self, other)

- schema: "bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseNot(ctx, out, self)

- schema: "bitwise_not_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiBitwiseNotInp(ctx, self)

- schema: "stack(Tensor[] tensors, int dim=0) -> Tensor"
  custom_code_at_the_beginning: |
    dim += dim < 0 ? tensors[0].sizes().size() : 0;
    auto num_tensors = tensors.size();
    auto shape = tensors[0].sizes();
    std::vector<int64_t> tmp;
    for (int i = 0; i < dim; i++) {
        tmp.push_back(shape[i]);
    }
    tmp.push_back(num_tensors);
    for (int i = dim; i < shape.size(); i++) {
        tmp.push_back(shape[i]);
    }
    const std::vector<int64_t>& const_tmp = tmp;
    shape = at::ArrayRef<int64_t>(const_tmp);
    auto out = at::empty({shape}, tensors[0].options());

    std::vector<diopiConstTensorHandle_t> diopiTensorHandles(tensors.size());
    for (size_t i = 0; i < tensors.size(); ++i) {
      diopiTensorHandles[i] = dipu::diopi_helper::toDiopiTensorHandle(tensors.at(i));
    }
  interface: diopiStack(ctx, out, diopiTensorHandles.data(), num_tensors, dim)

- schema: "stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    dim += dim < 0 ? tensors[0].sizes().size() : 0;
    std::vector<diopiConstTensorHandle_t> diopiTensorHandles(tensors.size());
    for (size_t i = 0; i < tensors.size(); ++i) {
      diopiTensorHandles[i] = dipu::diopi_helper::toDiopiTensorHandle(tensors.at(i));
    }
  interface: diopiStack(ctx, out, diopiTensorHandles.data(), tensors.size(), dim)

- schema: "sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)"
  custom_code_at_the_beginning: |
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
    auto values = at::empty(self.sizes(), self.options());
    auto indices = at::empty(self.sizes(), self.options().dtype(at::kLong));
  interface: diopiSort(ctx, values, indices, self, dim_, descending, nullptr)

- schema: "sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  custom_code_at_the_beginning: |
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
  interface: diopiSort(ctx, values, indices, self, dim_, descending, nullptr)

- schema: "sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  custom_code_at_the_beginning: |
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
    bool stable_ = stable.has_value() ? stable.value() : false;
    const bool *p = &stable_;
  interface: diopiSort(ctx, values, indices, self, dim_, descending, p)

- schema: "rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiRsqrt(ctx, out, self)

- schema: "uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiUniformInp(ctx, self, from, to, seed)

- schema: "tril(Tensor self, int diagonal=0) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiTril(ctx, out, self, diagonal)

- schema: "tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiTril(ctx, out, self, diagonal)

- schema: "multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
    if (self.dim() == 2){
      out = at::empty({self.size(0), num_samples}, self.options().dtype(at::kLong));
    }
    else if (self.dim() == 1) {
      out = at::empty({num_samples,}, self.options().dtype(at::kLong));
    }
  interface: diopiMultinomial(ctx, out, self, num_samples, replacement)

- schema: "multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMultinomial(ctx, out, self, num_samples, replacement)

- schema: "roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
    ::diopiSize_t diopi_shifts = toDiopiSize(shifts);
    ::diopiSize_t diopi_dims = toDiopiSize(dims);
  interface: diopiRoll(ctx, out, self, diopi_shifts, diopi_dims)

- schema: "leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor"
  print_func_call_info: True
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiLeakyRelu(ctx, out, self, negative_slope)

- schema: "mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor"
  custom_code_at_the_beginning: |
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor out;
    auto options = self.options();
    if (reductionDiopi == ReductionNone) {
      out = at::empty(self.sizes(), options);
    } else {
      out = at::empty({}, options);
    }
  interface: diopiMSELoss(ctx, out, self, target, reductionDiopi)

- schema: "mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
  interface: diopiMSELoss(ctx, out, self, target, reductionDiopi)

- schema: "clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)"
  interface: diopiClampInpScalar(ctx, self, min, max)

- schema: "clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiClampScalar(ctx, out, self, min, max)

- schema: "clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)"
  interface: diopiClampInp(ctx, self, min, max)

- schema: "clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiClamp(ctx, out, self, min, max)

- schema: "random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiRandomInp(ctx, self, 0, nullptr, seed)

- schema: "random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiRandomInp(ctx, self, 0, &to, seed)

- schema: "random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: "diopiRandomInp(ctx, self, from, to.has_value() ? &to.value() : nullptr, seed)"

- schema: "nonzero(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    at::Tensor out;
    diopiTensorHandle_t out_ptr = nullptr;
  interface: diopiNonzero(ctx, &out_ptr, self);
  custom_code_before_return: >
    dipu::getCurrentDIPUStream().synchronize();
    out = *reinterpret_cast<at::Tensor*>(out_ptr);

- schema: _softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiSoftmax(ctx, out, self, dim);

- schema: "_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiSoftmaxBackward(ctx, grad_input, grad_output, output, dim);


- schema: pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiPowTensor(ctx, out, self, exponent);

- schema: pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiPow(ctx, out, self, exponent);

- schema: pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)
  interface: diopiPowInpTensor(ctx, self, exponent);

- schema: pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)
  interface: diopiPowInp(ctx, self, exponent);

- schema: pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiPowScalar(ctx, out, self, exponent)

- schema: repeat(Tensor self, SymInt[] repeats) -> Tensor
  autocompare: disable
  custom_code_at_the_beginning: |
    std::vector<int64_t> output_size(repeats.size());
    for (int i = 0;i< repeats.size();++i) {
      output_size[i] = repeats.at(i).expect_int();
    }

    const auto& self_sizes = self.sizes();
    for (int i = self_sizes.size() - 1, j = output_size.size() - 1;i >= 0;i--, j--) {
      output_size[j] *= self_sizes.at(i);
    }

    at::Tensor out = at::empty(output_size, self.options());
  interface: diopiRepeat(ctx, out, self, repeats)

- schema: rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
    return dipu_sub_out(other, self, alpha, out);
  interface: diopiSub(ctx, out, other, self, alpha)

- schema: "unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor out, Tensor indices, Tensor counts)"
  custom_code_at_the_beginning: |
    at::Tensor out, indices, counts;
    diopiTensorHandle_t out_ptr = nullptr;
    diopiTensorHandle_t counts_ptr = nullptr;
  interface: diopiUnique(ctx, &out_ptr, self, &dim, sorted, return_counts, indices, &counts_ptr);
  custom_code_before_return: >
    dipu::getCurrentDIPUStream().synchronize();
    out = *reinterpret_cast<at::Tensor*>(out_ptr);
    if (return_counts) {
      counts = *reinterpret_cast<at::Tensor*>(counts_ptr);
    }

- schema: "_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor out, Tensor indices, Tensor counts)"
  custom_code_at_the_beginning: |
    at::Tensor out, indices, counts;
    diopiTensorHandle_t out_ptr = nullptr;
    diopiTensorHandle_t counts_ptr = nullptr;
  interface: diopiUnique(ctx, &out_ptr, self, nullptr, sorted, return_counts, indices, &counts_ptr);
  custom_code_before_return: >
    dipu::getCurrentDIPUStream().synchronize();
    out = *reinterpret_cast<at::Tensor*>(out_ptr);
    if (return_counts) {
      counts = *reinterpret_cast<at::Tensor*>(counts_ptr);
    }

- schema: "aten::cat.out(const at::ITensorListRef & tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    std::vector<diopiConstTensorHandle_t> diopiTensorHandles(tensors.size(), nullptr);
    std::transform(tensors.begin(), tensors.end(), diopiTensorHandles.begin(), [](const at::Tensor& tensor){
        return dipu::diopi_helper::toDiopiTensorHandle(tensor);
    });
  interface: diopiCat(ctx, out, diopiTensorHandles.data(), tensors.size(), dim);

- schema: "masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiMaskedFill(ctx, out, self, mask, value)

- schema: "masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)"
  interface: diopiMaskedFillInp(ctx, self, mask, value)

- schema: "masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiMaskedFillScalar(ctx, out, self, mask, value)

- schema: "masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)"
  interface: diopiMaskedFillInpScalar(ctx, self, mask, value)

- schema: "min(Tensor self) -> Tensor"
  use_diopi_adapter: False # camb adapter has problem
  custom_code_at_the_beginning: |
    auto out = at::empty({1}, self.options());
  interface: diopiMinAll(ctx, out, self)

- schema: "min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) min, Tensor(b!) min_indices)"
  custom_code_at_the_beginning: |
    dim += ((dim >= 0) ? 0 : self.sizes().size());
  interface: diopiMin(ctx, min, min_indices, self, dim)

- schema: "max(Tensor self) -> Tensor"
  use_diopi_adapter: False # camb adapter has problem
  custom_code_at_the_beginning: |
    auto out = at::empty({1}, self.options());
  interface: diopiMaxAll(ctx, out, self)

- schema: "maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMaximum(ctx, out, self, other)

- schema: "max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_indices) -> (Tensor(a!) max, Tensor(b!) max_indices)"
  custom_code_at_the_beginning: |
    dim += ((dim >= 0) ? 0 : self.sizes().size());
    auto output_size = self.sizes().vec();
  interface: diopiMax(ctx, max, max_indices, self, dim)

- schema: "addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)"
  interface:  diopiAddcdiv(ctx, out, self,tensor1,tensor2,value)

- schema: "addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)"
  interface:  diopiAddcdivInp(ctx,self , tensor1, tensor2, value)

- schema: "addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)"
  interface:  diopiAddcmul(ctx, out, self, tensor1, tensor2, value)

- schema: "addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)"
  interface:  diopiAddcmulInp(ctx,self,tensor1, tensor2, value)

- schema: "exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface:  diopiExp(ctx, out, self)

- schema: "exp_(Tensor(a!) self) -> Tensor(a!)"
  interface:  diopiExpInp(ctx, self)

- schema: "tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiTanh(ctx, out,self)

- schema: "tanh_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiTanhInp(ctx,self)

- schema: "argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const int64_t* ptr = dim.has_value() ? (&dim.value()): nullptr;
  interface: diopiArgmax(ctx, out, self, ptr, keepdim)

- schema: "masked_select(Tensor self, Tensor mask) -> Tensor"
  custom_code_at_the_beginning: |
    at::Tensor out;
    diopiTensorHandle_t out_ptr = nullptr;
  interface: diopiMaskedSelect(ctx, &out_ptr, self, mask);
  custom_code_before_return: >
    dipu::getCurrentDIPUStream().synchronize();
    out = *reinterpret_cast<at::Tensor*>(out_ptr);

- schema: "masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMaskedSelect(ctx, &out, self, mask);

- schema: "baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    double betaDouble = beta.toDouble();
    double alphaDouble = alpha.toDouble();
  interface: diopiBaddbmm(ctx, out, self, batch1, batch2, betaDouble, alphaDouble)

- schema: "baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    double betaDouble = beta.toDouble();
    double alphaDouble = alpha.toDouble();
  interface: diopiBaddbmmInp(ctx, self, batch1, batch2, betaDouble, alphaDouble)

- schema: "floor_divide(Tensor self, Tensor other) -> Tensor"
  custom_code_at_the_beginning: |
    at::Tensor out = at::empty_like(self);
    out = dipu_div_out(self,other,out);
  interface: diopiFloorInp(ctx,out)

- schema: "where.self(Tensor condition, Tensor self, Tensor other) -> Tensor"
  custom_code_at_the_beginning: |
    auto shape = at::infer_size(condition.sizes(), self.sizes());
    shape = at::infer_size(shape, other.sizes());
    auto out = at::empty(shape, self.options());
  interface: diopiWhere(ctx, out, condition,self, other)

- schema: "gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const auto str =  std::string(approximate);
    const char* appr = str.data();
  interface: diopiGelu(ctx, out, self, appr)

- schema: "hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor"
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  custom_code_before_call_diopi: >
    min_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(min_val, at::kDouble);
    max_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(max_val, at::kDouble);

  interface: diopiHardtanh(ctx, out, self, min_val, max_val)

- schema: "hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)"
  custom_code_before_call_diopi: >
    min_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(min_val, at::kDouble);
    max_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(max_val, at::kDouble);
  interface: diopiHardtanhInp(ctx, self, min_val, max_val)

- schema: "hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_before_call_diopi: >
    min_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(min_val, at::kDouble);
    max_valDiopiScalar = dipu::diopi_helper::toDiopiScalar(max_val, at::kDouble);

  interface: diopiHardtanh(ctx, out, self, min_val, max_val)

- schema: "hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiHardtanhBackward(ctx, grad_input, grad_output, self, min_val, max_val)

- schema: "sin(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiSin(ctx, out, self)

- schema: "sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSin(ctx, out, self)

- schema: "sin_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiSinInp(ctx, self)

- schema: "cos(Tensor self) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
  interface: diopiCos(ctx, out, self)

- schema: "cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiCos(ctx, out, self)

- schema: "cos_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiCosInp(ctx, self)

- schema: "bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBmm(ctx, out, self, mat2)

- schema: "silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSilu(ctx, out, self)

- schema: "reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiReciprocal(ctx, out, self)

- schema: "normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)"
  autocompare: disable
  interface: diopiNormalInp(ctx, self, mean, std)

- schema: "mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMm(ctx, out, self, mat2)

- schema: "matmul(Tensor self, Tensor other) -> Tensor"
  custom_code_at_the_beginning: |
    const auto shapeA = self.sizes();
    const auto shapeB = other.sizes();
    const int64_t nA= shapeA.size();
    const int64_t nB = shapeB.size();
    std::vector<int64_t> output_shape;
    if (nA == nB && nB == 2) {
        output_shape = {shapeA[0], shapeB[1]};
    } else if (nA == 1 && nB == 2) {
      output_shape = {shapeB[1]};
    } else if (nA == 2 && nB == 1) {
      output_shape = {shapeA[0]};
    } else if (nA > 2 && nB == 1) {
      output_shape = std::vector<int64_t>(shapeA.begin(), shapeA.end() - 1);
    } else if (nA == 1 && nB > 2) {
      output_shape = std::vector<int64_t>(shapeB.begin(), shapeB.end());
      output_shape.erase(output_shape.end() - 2);
    } else if (nA >= 2 && nB >= 2) {
      const int64_t nC = std::max(nA, nB);
      output_shape = std::vector<int64_t>(nC, 1);
      output_shape[output_shape.size() - 1] = shapeB[nB - 1];
      output_shape[output_shape.size() - 2] = shapeA[nA - 2];
      for (int i = 3; i <= nC; ++i) {
        int dim = nC - i;
        if (nA - i >= 0) {
          output_shape[dim] = std::max(output_shape[dim], shapeA[nA - i]);
        }
        if (nB - i >= 0) {
          output_shape[dim] = std::max(output_shape[dim], shapeB[nB - i]);
        }
      }
    }

    auto out = at::empty(output_shape, self.options());

  interface: diopiMatmul(ctx, out, self, other)

- schema: "matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMatmul(ctx, out, self, other)

- schema: "cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiCumsum( ctx,  out, self, dim)

- schema: "flip(Tensor self, int[] dims) -> Tensor"
  custom_code_at_the_beginning: |
    auto out = at::empty_like(self);
    ::diopiSize_t diopi_size = toDiopiSize(dims);
  interface: diopiFlip(ctx, out,self,diopi_size)

- schema: "linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLinspace(ctx,out,  start, end,  steps)

- schema: "gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiGather(ctx,out, self,  dim,  index)

- schema: "index_select(Tensor self, int dim, Tensor index) -> Tensor"
  custom_code_at_the_beginning: |
    auto shape = self.sizes();
    std::vector<int64_t> output_shape(shape.begin(), shape.end());
    dim += dim >= 0 ? 0 : shape.size();
    output_shape[dim] = index.numel();
    auto out = at::empty({output_shape}, self.options());
  interface: diopiIndexSelect(ctx, out, self, dim, index)

- schema: "sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSigmoid(ctx,out,self)

- schema: "clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiClampMinScalar(ctx, out, self, min)

- schema: "clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiClampMaxScalar(ctx, out, self, max)

- schema: "minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMinimum(ctx,out, self, other)

- schema: "scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    const auto str =  std::string("None");
    const char* appr = str.data();
  interface: diopiScatterScalar(ctx,out, self, dim, value, index, appr)

- schema: "remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiRemainderTensor(ctx,out, self, other)

- schema: "norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: |
    ::diopiSize_t dimDiopiSize = toDiopiSize(dim);
  interface: diopiNorm(ctx, out, self, p, dimDiopiSize);

- schema: "floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface:  diopiFloor(ctx, out,self);

