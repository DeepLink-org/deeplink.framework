- schema: "exampleop.overloadname(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  register_op: False  # Whether generate registe code for this op, default value is True
  print_func_call_info: False # wheher generate code that prints function call information
  dummy_call_diopi: False # Does not generate code that actually calls the diopi function, defalut value is False
  custom_code_at_the_beginning: "/* Here can be a piece of c++ code at the begining*/"
  custom_code_before_call_diopi: >
    std::cout << "self:" << self << std::endl;
    std::cout << "other:" << other << std::endl;
  custom_code_before_return: >
    dipu::getCurrentDIPUStream().synchronize();
    std::cout << "out:" << out << std::endl;
  interface: diopiAddScalar(ctx, out, self, other, alpha)

- schema: "aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAddScalar(ctx, out, self, other, alpha)

- schema: "aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_add_scalar_out(self, other.item(), alpha, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_add_scalar_out(other, self.item(), alpha, out);
    }
  interface: diopiAdd(ctx, out, self, other, alpha)

- schema: "aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSubScalar(ctx, out, self, other, alpha)

- schema: "aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_sub_scalar_out(self, other.item(), alpha, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_sub_scalar_out(other, self.item(), alpha, out);
    }
  interface: diopiSub(ctx, out, self, other, alpha)

- schema: "div.Scalar(Tensor self, Scalar other) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiDivScalar(ctx, out, self, other, RoundModeNone)

- schema: "div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiDivInpScalar(ctx, self, other, RoundModeNone)

- schema: "div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div__scalar(self, other.item());
    }
  interface: diopiDivInp(ctx, self, other, RoundModeNone)

- schema: "aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_div_scalar(self, other.item());
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_div_scalar(other, self.item());
        return out;
    }
  interface: diopiDiv(ctx, out, self, other, RoundModeNone)

- schema: "aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDivScalar(ctx, out, self, other, mode)

- schema: "aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div_scalar_mode_out(self, other.item(), rounding_mode, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_div_scalar_mode_out(other, self.item(), rounding_mode, out);
    }
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDiv(ctx, out, self, other, mode)

- schema: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
  interface: diopiFill(ctx, self, value)

- schema: "mul.Scalar(Tensor self, Scalar other) -> Tensor"
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  interface: diopiMulScalar(ctx, out, self, other)

- schema: "mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiMulInpScalar(ctx, self, other)

- schema: "mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_mul__scalar(self, other.item());
    }
  interface: diopiMulInp(ctx, self, other)

- schema: "mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_mul_scalar(self, other.item());
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_mul_scalar(other, self.item());
        return out;
    }
  interface: diopiMul(ctx, out, self, other)

- schema: "aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  interface: diopiBatchNorm(ctx, out, save_mean, save_invstd, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);

- schema: "aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)"
  custom_code_at_the_beginning: >
    const int64_t dim_c = input.size(1);
    auto out0 = at::empty_like(input);
    auto options = input.options().dtype(at::kFloat);
    auto out1 = at::empty({dim_c}, options);
    auto out2 = at::empty({dim_c}, options);
  interface: diopiBatchNorm(ctx, out0, out1, out2, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);

- schema: "native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  custom_code_at_the_beginning: >
    int64_t dim_c = input.size(1);
    auto options = input.options().dtype(at::kFloat);
    at::Tensor out0 = at::empty(input.sizes(), input.options());
    at::Tensor out1 = at::empty({dim_c}, options);
    at::Tensor out2 = at::empty({dim_c}, options);
  interface: diopiBatchNormBackward(ctx, out0, out1, out2, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps)

- schema: "adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAdaptiveAvgPool2d(ctx, out, self, output_size)

- schema: "_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor"
  custom_code_at_the_beginning: >
    TORCH_CHECK(output_size.size() == 2, __func__, ":", __FILE__, ":", __LINE__,
        " output_size should equal 2, size is ", output_size.size());
    auto out_tensor_size = self.sizes().vec();
    out_tensor_size[self.dim() - 2] = output_size[0].expect_int();
    out_tensor_size[self.dim() - 1] = output_size[1].expect_int();
    at::Tensor out = at::empty(out_tensor_size, self.options());
  interface: diopiAdaptiveAvgPool2d(ctx, out, self, output_size)

- schema: "_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiAdaptiveAvgPool2dBackward(ctx, out, grad_output, self);

- schema: "relu_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiReluInp(ctx, self)

- schema: "relu(Tensor self) -> Tensor"
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  interface: diopiRelu(ctx, out, self)

- schema: "randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiRandperm(ctx, out, n, 0)

- schema: "randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiRandperm(ctx, out, n, seed)

- schema: "aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    ::diopiSize_t diopi_size = toDiopiSize(dim);
  interface: diopiSum(ctx, out, self, diopi_size)

- schema: "aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiFloor(ctx, out, self)

- schema: "aten::floor_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiFloorInp(ctx, self)

- schema: "aten::max(Tensor self) -> Tensor"
  custom_code_at_the_beginning: >
    Tensor out = at::empty({1}, self.options());
    diopiTensorHandle_t max =reinterpret_cast<diopiTensorHandle_t>(&out);
    auto empty_indices = at::empty({1}, self.options().dtype(at::kLong));
    diopiTensorHandle_t max_indices = reinterpret_cast<diopiTensorHandle_t>(&empty_indices);
    diopiConstTensorHandle_t input = toDiopiTensorHandle(self);
    int64_t dim = 0;
  interface: diopiMax(ctx, max, max_indices, input, dim)

- schema: "aten::min(Tensor self) -> Tensor"
  custom_code_at_the_beginning: >
    Tensor out = at::empty({1}, self.options());
    diopiTensorHandle_t min =reinterpret_cast<diopiTensorHandle_t>(&out);
    auto empty_indices = at::empty({1}, self.options().dtype(at::kLong));
    diopiTensorHandle_t min_indices = reinterpret_cast<diopiTensorHandle_t>(&empty_indices);
    diopiConstTensorHandle_t input = toDiopiTensorHandle(self);
    int64_t dim = 0;
  interface: diopiMin(ctx, min, min_indices, input, dim)
